# Directory: src/prompting/

## Overview
The prompting directory provides a comprehensive suite of utilities for advanced prompt engineering in LLM applications. It represents a significant refactoring from the original utils directory, with a clearer separation of concerns and more descriptive naming. The module is organized around the prompt engineering lifecycle: template rendering, content preparation, and response parsing.

## Key Components

### index.ts
## Overview
This file serves as the central export point for all prompting utilities in the genai-lite library. It provides a clean, organized API surface by re-exporting functions from the main modules: template rendering, content preparation, and response parsing.

## Key Components
- **Template Rendering**: 
  - `renderTemplate` from `./template` - Dynamic text generation with variables and conditionals
- **Content Preparation**:
  - `countTokens` from `./content` - Token counting for cost estimation
  - `getSmartPreview` from `./content` - Intelligent content truncation
  - `extractRandomVariables` from `./content` - Few-shot example extraction
- **Response Parsing**:
  - `parseStructuredContent` from `./parser` - Extract structured data from responses
  - `parseRoleTags` from `./parser` - Parse role-tagged templates into LLM messages
  - `extractInitialTaggedContent` from `./parser` - Extract thinking blocks from response start

## Architecture & Design
The file follows a barrel export pattern, organizing exports by their functional area. This structure makes it clear which utility serves which purpose in the prompt engineering workflow.

## See Also
- Individual module files for implementation details
- Main `src/index.ts` for how these utilities are exposed in the library's public API

### template.ts
## Overview
This module provides the core template rendering engine, migrated from the original templateEngine.ts. It implements a sophisticated micro-templating system that supports variable substitution and conditional rendering, making it the foundation for dynamic prompt generation.

## Key Components
- **renderTemplate(template, variables)**: Main function processing templates with variable substitution and conditionals
- **processExpression**: Internal function handling variable substitution and ternary logic
- **Custom Parser**: Depth-aware parser for nested template expressions

## Architecture & Design
The module uses a custom parsing approach to handle complex nested templates:
- **Depth Tracking**: Monitors brace depth for correct `{{}}` matching
- **Backtick Support**: Special handling for multi-line strings
- **Intelligent Newlines**: Removes empty lines when substitution results are empty
- **Recursive Processing**: Supports nested variables in conditional branches

## Supported Syntax
- Simple substitution: `{{ variableName }}`
- Conditionals: `{{ condition ? `true` : `false` }}`
- Optional false: `{{ condition ? `true` : `` }}`
- Nested variables: `{{ show ? `Hello, {{name}}!` : `Guest` }}`

## Dependencies
- **Internal**: None
- **External**: None (pure TypeScript)

## Usage Examples
```typescript
import { renderTemplate } from 'genai-lite/prompting';

const prompt = renderTemplate(
  'Analyze {{ language }} code{{ hasContext ? `: {{context}}` : `` }}',
  { language: 'TypeScript', hasContext: true, context: 'React hooks' }
);
```

### content.ts
## Overview
This module consolidates content preparation utilities from the original prompt.ts and parts of promptBuilder.ts. It provides essential functions for preparing raw text content before it's assembled into prompts, including token counting, smart previews, and random example extraction.

## Key Components
- **countTokens(text, model)**: Counts tokens using model-specific tokenizers
- **getSmartPreview(content, config)**: Creates intelligent content previews
- **extractRandomVariables(content, options)**: Extracts and shuffles examples for few-shot learning
- **tokenizerCache**: Map-based cache for tokenizer instances

## Architecture & Design
The module implements several optimization strategies:
- **Tokenizer Caching**: Reuses tokenizer instances for performance
- **Graceful Fallback**: Falls back to character estimation if tokenization fails
- **Natural Boundaries**: Preview function seeks empty lines for clean truncation
- **Fisher-Yates Shuffle**: Implements proper randomization for examples

## Dependencies
- **Internal**: None
- **External**: `js-tiktoken` for accurate token counting

## Integration Points
Used throughout the library for:
- API request preparation and validation
- Content display in UIs
- Few-shot prompt construction
- Token-based cost estimation

### parser.ts
## Overview
This module provides utilities for parsing structured content from LLM responses and parsing role-tagged templates. It consolidates functionality previously spread across multiple modules, now including the role tag parsing that enables the unified prompt creation workflow.

## Key Components
- **parseStructuredContent(content, tags)**: Extracts content from XML-style tags
- **parseRoleTags(template)**: Parses role-tagged templates (SYSTEM, USER, ASSISTANT) into message arrays
- **extractInitialTaggedContent(content, tagName)**: Extracts content from a specific tag at the beginning of a string

## Architecture & Design
The parser is designed for flexibility and robustness:
- **Tag Order Awareness**: Processes tags in specified order
- **Flexible Matching**: Handles both closed and unclosed tags
- **Graceful Degradation**: Returns empty strings for missing tags
- **Role Tag Parsing**: Converts SYSTEM/USER/ASSISTANT tags to structured messages
- **Simple API**: Three focused functions with clear inputs/outputs
- **Specialized Extraction**: extractInitialTaggedContent specifically targets tags at content start

## Use Cases
- Extracting structured reasoning from LLM responses
- Parsing role-tagged templates for message creation
- Automatic thinking block extraction for response processing
- Parsing multi-part answers (thinking, code, explanation)
- Handling streaming responses with incomplete tags
- Building robust LLM output pipelines
- Automatic extraction of thinking blocks for post-processing

## Dependencies
- **Internal**: None
- **External**: None

## Usage Examples
```typescript
import { parseStructuredContent } from 'genai-lite/prompting';

const response = `
<ANALYSIS>The code has a memory leak</ANALYSIS>
<SOLUTION>Use proper cleanup in useEffect</SOLUTION>
`;

const parsed = parseStructuredContent(response, ['ANALYSIS', 'SOLUTION']);
// parsed.ANALYSIS = "The code has a memory leak"
// parsed.SOLUTION = "Use proper cleanup in useEffect"

// Extract thinking from the beginning of a response
const llmResponse = `<thinking>Let me analyze this step by step...</thinking>The answer is 42.`;
const { extracted, remaining } = extractInitialTaggedContent(llmResponse, 'thinking');
// extracted = "Let me analyze this step by step..."
// remaining = "The answer is 42."
```

## Architecture
The prompting directory follows a clear architectural pattern aligned with the prompt engineering workflow:

1. **Template Layer** (`template.ts`): Provides the foundational rendering engine
2. **Content Layer** (`content.ts`): Prepares raw content for use in prompts
3. **Builder Layer** (`builder.ts`): Assembles structured messages for LLMs
4. **Parser Layer** (`parser.ts`): Extracts structured data from responses

This layered approach provides:
- **Clear Separation**: Each module has a distinct responsibility
- **Workflow Alignment**: Structure mirrors the prompt engineering process
- **Reusability**: Each layer can be used independently
- **Maintainability**: Changes are localized to specific concerns

## Design Philosophy
The prompting module follows these principles:
- **Separation of Concerns**: Template rendering, content preparation, and parsing are independent
- **Composability**: Each utility can be used standalone or combined
- **Type Safety**: Full TypeScript support throughout
- **Flexibility**: Support for various prompting patterns and use cases

## Dependencies
- **Internal**: The parser module depends on LLM types for message structures
- **External**: Only js-tiktoken for token counting

## Integration Points
The prompting module serves as a foundational layer for:
- **LLM Service**: Message preparation and response parsing
- **Client Applications**: Dynamic prompt generation
- **Cost Management**: Token counting and estimation
- **Content Display**: Smart previews and truncation
- **Few-shot Learning**: Example extraction and randomization

## Testing
Each module has comprehensive test coverage:
- **template.test.ts**: 20+ tests covering all template features
- **content.test.ts**: Tests for token counting, previews, and random extraction
- **parser.test.ts**: Structured content extraction and role tag parsing scenarios

## Usage Examples
```typescript
import { 
  renderTemplate,
  countTokens,
  getSmartPreview,
  parseStructuredContent,
  parseRoleTags 
} from 'genai-lite/prompting';

// Complete prompt engineering workflow
const template = `
<SYSTEM>You are a code review expert.</SYSTEM>
<USER>Review this {{language}} code:
\`\`\`{{language}}
{{code}}
\`\`\`
</USER>
`;

// 1. Prepare content
const codeTokens = countTokens(code, 'gpt-4');
if (codeTokens > 1000) {
  code = getSmartPreview(code, { minLines: 50, maxLines: 100 });
}

// 2. Build messages (using LLMService.createMessages or standalone utilities)
// Option A: Using LLMService (recommended)
const { messages } = await llmService.createMessages({
  template,
  variables: { language: 'TypeScript', code }
});

// Option B: Using standalone utilities
const rendered = renderTemplate(template, { language: 'TypeScript', code });
const messages = parseRoleTags(rendered);

// 3. Send to LLM and parse response
const response = await llmService.sendMessage({ messages });
const parsed = parseStructuredContent(llmResponse, [
  'ISSUES', 
  'SUGGESTIONS', 
  'SECURITY_CONCERNS'
]);
```

## See Also
- **Parent Directory**: src/ - Main source directory
- **Related Modules**:
  - src/llm/ - Uses these utilities for message handling
  - src/index.ts - Exports these utilities in public API
- **Migration Notes**: Applications using old paths need updates