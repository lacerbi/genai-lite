# Directory: src/

## Overview
The src directory is the heart of the genai-lite library, containing the main entry point and foundational type definitions. This directory establishes the public API surface and core abstractions that make the library flexible and environment-agnostic. It serves as the boundary between the library's internal implementation and its consumers, providing a clean, well-organized interface for interacting with multiple AI providers through a unified API. The directory now includes a sophisticated model presets system that allows applications to ship with pre-configured model settings while maintaining full flexibility for customization.

## Key Components
### index.ts
## Overview
This file serves as the main entry point for the genai-lite library, defining its public API surface. It aggregates and re-exports all the essential components, types, and utilities that consumers of the library need to interact with various generative AI providers through a unified interface.

## Key Components
- **ApiKeyProvider type**: Core type for implementing custom API key retrieval strategies
- **LLMService class**: Main service class that orchestrates all LLM operations across different providers
  - Includes new `createMessages()` method for unified prompt creation with model-aware template rendering
  - Enhanced `sendMessage()` with preset support via `LLMChatRequestWithPreset`
- **LLMServiceOptions & PresetMode**: Configuration options for customizing the LLM service with model presets
- **ModelPreset type**: Interface for defining pre-configured model settings for common use cases
- **LLM types module**: All request/response/configuration types from the LLM module, including reasoning settings
  - `LLMChatRequest` & `LLMChatRequestWithPreset`: Request types for traditional and preset-based messaging
  - `ModelContext`: Type for model context information used in template rendering
  - `LLMThinkingExtractionSettings`: Settings for automatic thinking extraction from responses
- **Client adapter types**: Type definitions for LLM client adapter implementations
- **fromEnvironment function**: Built-in API key provider that reads from environment variables

## Architecture & Design
The file follows a clean export pattern, organizing exports into logical groups with clear comments. It uses a combination of named exports and re-exports to provide a comprehensive yet organized API surface. The design allows consumers to import everything they need from a single entry point while maintaining clear separation of concerns.

## Dependencies
- **Internal**: 
  - `./types` - Core type definitions
  - `./llm/LLMService` - Main service implementation
  - `./llm/types` - LLM-specific types
  - `./llm/clients/types` - Client adapter types
  - `./providers/fromEnvironment` - Environment variable provider
- **External**: None (pure TypeScript module exports)

## Integration Points
This file serves as the central integration point for all library consumers. Applications using genai-lite will import from this file to access the LLM service, configure API key providers, and work with the type system. It acts as the public API boundary, hiding internal implementation details while exposing a clean, documented interface.

## Usage Examples
```typescript
// Import the main service and types including new types
import { 
  LLMService, 
  fromEnvironment, 
  ApiKeyProvider, 
  ModelPreset,
  LLMChatRequestWithPreset,
  ModelContext 
} from 'genai-lite';

// Use the built-in environment provider with default presets
const service = new LLMService(fromEnvironment);

// Send message using preset
const response = await service.sendMessage({
  presetId: 'anthropic-claude-3-7-sonnet-20250219-thinking',
  messages: [{ role: 'user', content: 'Solve this problem step by step...' }]
});

// Use unified prompt creation with model-aware template rendering
const { messages, modelContext } = await service.createMessages({
  template: `<SYSTEM>{{ thinking_enabled ? "Think step-by-step:" : "Analyze:" }}</SYSTEM>
<USER>{{ question }}</USER>`,
  variables: { question: 'What is quantum computing?' },
  presetId: 'anthropic-claude-3-7-sonnet-20250219-thinking'
});

// Customize with your own presets
const customPresets: ModelPreset[] = [
  {
    id: 'my-gpt4-preset',
    displayName: 'GPT-4 Creative',
    providerId: 'openai',
    modelId: 'gpt-4',
    settings: { temperature: 0.8 }
  }
];

const serviceWithPresets = new LLMService(fromEnvironment, {
  presets: customPresets,
  presetMode: 'extend' // or 'replace' to use only custom presets
});
```

## See Also
- `src/llm/LLMService.ts` - Main service implementation
- `src/types.ts` - Core type definitions
- `src/providers/fromEnvironment.ts` - Environment variable provider implementation
- `src/llm/config.ts` - Provider and model configurations

### types.ts
## Overview
This file defines the fundamental type contract for API key management in the genai-lite library. It establishes a flexible, provider-agnostic pattern for retrieving API keys that allows the library to work in any JavaScript environment without being tied to specific storage mechanisms.

## Key Components
- **ApiKeyProvider**: A function type that takes a provider ID string and returns a Promise resolving to either an API key string or null if no key is found

## Architecture & Design
The design follows a functional programming pattern with an async function signature, enabling:
- Asynchronous key retrieval from various sources (environment variables, databases, vaults, etc.)
- Provider-specific key lookup based on the providerId parameter
- Graceful handling of missing keys through the nullable return type

## Dependencies
- **Internal**: None - this is a foundational type definition
- **External**: None - uses only TypeScript's built-in types

## Integration Points
This type is fundamental to the library's architecture:
- Used by `LLMService` constructor to accept custom key providers
- Implemented by `fromEnvironment` provider for environment variable lookup
- Can be implemented by consumers to integrate with their own key storage solutions

## Usage Examples
```typescript
// Built-in environment provider
const envProvider: ApiKeyProvider = async (providerId) => {
  return process.env[`${providerId.toUpperCase()}_API_KEY`] || null;
};

// Custom database provider
const dbProvider: ApiKeyProvider = async (providerId) => {
  const key = await database.getApiKey(providerId);
  return key || null;
};
```

## See Also
- `src/providers/fromEnvironment.ts` - Built-in implementation of this type
- `src/llm/LLMService.ts` - Main consumer of ApiKeyProvider
- `src/index.ts` - Exports this type as part of the public API

## Architecture
The src directory establishes a clear architectural boundary with `index.ts` serving as the public API gateway and `types.ts` providing the foundational abstractions. This design promotes:
- **Encapsulation**: Internal implementation details are hidden behind the public API
- **Flexibility**: The ApiKeyProvider pattern allows the library to work in any environment
- **Maintainability**: Clear separation between public API and internal modules
- **Type Safety**: Strong typing throughout with TypeScript

## Internal Dependencies
The files in this directory form the foundation of the library's dependency graph:
- `index.ts` depends on all major internal modules to re-export them
- `types.ts` has no dependencies, serving as a pure type definition module

## External Dependencies
This directory has no external dependencies, relying only on TypeScript's type system and module system.

## Integration Points
The src directory is the primary integration point for all library consumers:
- **Library consumers** import from `index.ts` to access all functionality
- **Internal modules** implement the types defined in `types.ts`
- **Build tools** use `index.ts` as the main entry point for compilation

## Usage Examples
```typescript
// Complete example of using the library
import { LLMService, fromEnvironment, LLMChatRequest } from 'genai-lite';

// Initialize the service
const llmService = new LLMService(fromEnvironment);

// Create a chat request
const request: LLMChatRequest = {
  providerId: 'openai',
  modelId: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello, world!' }]
};

// Send the request
const response = await llmService.sendMessage(request);
console.log(response);
```

## See Also
- **Parent Directory**: [root] - Contains project configuration and documentation
- **Related Directories**:
  - `src/llm/` - Core LLM service implementation and configurations
  - `src/providers/` - API key provider implementations
  - `src/prompting/` - Prompt engineering utilities (template rendering, content preparation, message building, response parsing)
- **Key Consumers**: All applications and services that depend on genai-lite import from this directory