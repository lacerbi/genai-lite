# Directory: src/llm/clients/

## Overview
The clients directory implements the Adapter pattern for integrating multiple AI provider APIs into a unified interface. This directory contains provider-specific client adapters (OpenAI, Anthropic, Google Gemini) that translate between the library's standardized request/response format and each provider's unique API requirements. It also includes shared utilities for error handling, comprehensive type definitions, and a mock adapter for testing. Each adapter handles authentication, request formatting, API communication, response normalization, and error mapping to ensure consistent behavior across all providers.

## Key Components
### types.ts
## Overview
This file establishes the core interface contract for LLM client adapters in the genai-lite library. It defines the standardized interface that all AI provider adapters (OpenAI, Anthropic, Google, Mistral, etc.) must implement to ensure consistent behavior across different providers. The file also includes helper types and error code constants to maintain consistency in error handling across adapters.

## Key Components
- **InternalLLMChatRequest**: Extended interface that ensures all settings have required values, removing the need for adapters to handle undefined values
- **ILLMClientAdapter**: Main interface defining the contract with methods:
  - `sendMessage()`: Core method for sending chat requests to LLM providers
  - `validateApiKey()`: Optional method for API key format validation
  - `getAdapterInfo()`: Optional method for provider metadata
- **ADAPTER_ERROR_CODES**: Constant object defining standardized error codes for consistent error handling
- **AdapterErrorCode**: Type helper derived from the error codes constant

## Architecture & Design
The design follows the Adapter pattern, allowing the library to support multiple AI providers through a common interface. Key architectural decisions include:
- Using TypeScript's `Required<T>` utility type to ensure settings are fully populated before reaching adapters
- Making error handling methods optional to allow flexibility while encouraging consistency
- Returning either success or failure responses rather than throwing errors, promoting predictable error handling
- Providing standardized error codes to ensure consistent error categorization across providers

## Dependencies
- **Internal**: 
  - `../types` - Core LLM types including LLMChatRequest, LLMResponse, LLMFailureResponse, and LLMSettings
- **External**: None (pure TypeScript type definitions)

## Integration Points
This interface is implemented by all provider-specific adapters:
- `OpenAIClientAdapter.ts`
- `AnthropicClientAdapter.ts`
- `GeminiClientAdapter.ts`
- `MistralClientAdapter.ts`

The LLMService uses this interface to interact with adapters in a provider-agnostic way.

## Usage Examples
```typescript
// Example adapter implementation
class OpenAIClientAdapter implements ILLMClientAdapter {
  async sendMessage(
    request: InternalLLMChatRequest,
    apiKey: string
  ): Promise<LLMResponse | LLMFailureResponse> {
    try {
      // Provider-specific API call
      const result = await openai.chat.completions.create({...});
      return { success: true, content: result.choices[0].message.content };
    } catch (error) {
      return {
        success: false,
        error: error.message,
        errorCode: ADAPTER_ERROR_CODES.PROVIDER_ERROR
      };
    }
  }
}
```

## See Also
- `src/llm/clients/OpenAIClientAdapter.ts` - Example implementation
- `src/llm/LLMService.ts` - Main service that uses these adapters
- `src/llm/types.ts` - Core types used by this interface
- `src/llm/clients/adapterErrorUtils.ts` - Error handling utilities for adapters

### adapterErrorUtils.ts
## Overview
This utility file provides centralized error mapping functionality for LLM client adapters. It standardizes error handling across different AI providers (OpenAI, Anthropic, etc.) by converting provider-specific errors, HTTP status codes, and network errors into a consistent format with standardized error codes and types.

## Key Components
- **MappedErrorDetails interface**: Defines the structure for standardized error information including errorCode, errorMessage, errorType, and optional status
- **getCommonMappedErrorDetails function**: Main utility function that analyzes error objects and maps them to standardized error codes based on HTTP status, network conditions, or error type

## Architecture & Design
The file implements a centralized error mapping pattern to reduce code duplication across provider adapters. It uses a switch-case structure for HTTP status code mapping and conditional checks for network errors. The design allows individual adapters to further refine mappings for provider-specific cases while maintaining a consistent baseline.

## Dependencies
- **Internal**: 
  - `./types` - Imports ADAPTER_ERROR_CODES constant and AdapterErrorCode type
- **External**: None (pure TypeScript utility)

## Integration Points
This utility is used by all LLM provider client adapters (OpenAI, Anthropic, Gemini, Mistral) to standardize their error responses. Adapters call this function when handling errors from their respective provider SDKs, then can optionally refine the mappings for provider-specific error patterns.

## Usage Examples
```typescript
// In a provider adapter's error handler
try {
  const response = await openai.chat.completions.create(request);
} catch (error) {
  const mappedError = getCommonMappedErrorDetails(error);
  // Optionally refine for provider-specific cases
  if (error.status === 400 && error.message.includes('context_length_exceeded')) {
    mappedError.errorCode = ADAPTER_ERROR_CODES.CONTEXT_LENGTH_EXCEEDED;
  }
  return mappedError;
}
```

## See Also
- `src/llm/clients/types.ts` - Defines AdapterErrorCode enum and related types
- `src/llm/clients/OpenAIClientAdapter.ts` - Example adapter using this utility
- `src/llm/clients/AnthropicClientAdapter.ts` - Another adapter using this utility
- All other provider adapter files that depend on this error mapping

### OpenAIClientAdapter.ts
## Overview
The OpenAIClientAdapter implements the ILLMClientAdapter interface to provide integration with OpenAI's chat completions API. It handles request formatting, authentication, API communication, and response normalization, converting between the application's internal format and OpenAI's specific API requirements.

## Key Components
- **OpenAIClientAdapter class**: Main adapter implementing ILLMClientAdapter interface
  - `sendMessage()`: Sends chat requests to OpenAI API and returns standardized responses
  - `validateApiKey()`: Validates OpenAI API key format (must start with 'sk-' and be â‰¥20 chars)
  - `getAdapterInfo()`: Returns adapter metadata
- **Private methods**:
  - `formatMessages()`: Converts internal message format to OpenAI's expected format
  - `createSuccessResponse()`: Transforms OpenAI completion to standardized LLMResponse
  - `createErrorResponse()`: Maps OpenAI errors to standardized LLMFailureResponse

## Architecture & Design
- Implements adapter pattern for provider abstraction
- Uses OpenAI's official Node.js client library
- Supports custom base URLs for OpenAI-compatible APIs
- Comprehensive error handling with specific error code mapping
- Logging for debugging API calls and responses

## Dependencies
- **Internal**: 
  - `../types` (LLMResponse, LLMFailureResponse types)
  - `./types` (ILLMClientAdapter, InternalLLMChatRequest, AdapterErrorCode)
  - `./adapterErrorUtils` (getCommonMappedErrorDetails utility)
- **External**: 
  - `openai` - Official OpenAI Node.js client library

## Integration Points
- Implements standardized ILLMClientAdapter interface for use by LLMService
- Converts between internal message format and OpenAI's chat completion format
- Maps OpenAI-specific errors to standardized error codes (e.g., CONTEXT_LENGTH_EXCEEDED, CONTENT_FILTER)
- Supports all OpenAI chat parameters: temperature, max_tokens, top_p, stop sequences, penalties

## Usage Examples
```typescript
// Create adapter with custom base URL
const adapter = new OpenAIClientAdapter({ 
  baseURL: 'https://custom-openai-api.com' 
});

// Send message through adapter
const response = await adapter.sendMessage({
  providerId: 'openai',
  modelId: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello!' }],
  settings: {
    temperature: 0.7,
    maxTokens: 1000,
    // ... other settings
  }
}, apiKey);
```

## See Also
- `../LLMService.ts` - Main service that uses this adapter
- `./types.ts` - Interface definitions and error codes
- `./adapterErrorUtils.ts` - Shared error mapping utilities
- Other adapters: AnthropicClientAdapter, GeminiClientAdapter, MistralClientAdapter

### AnthropicClientAdapter.ts
## Overview
The AnthropicClientAdapter implements the ILLMClientAdapter interface to provide integration with Anthropic's Claude models. It handles the specific requirements of Anthropic's Messages API, including proper message formatting, system message positioning, and the conversion between standardized LLM formats and Anthropic's proprietary API structure.

## Key Components
- **AnthropicClientAdapter class**: Main adapter implementing ILLMClientAdapter
  - `sendMessage()`: Sends chat completion requests to Anthropic's API
  - `validateApiKey()`: Validates Anthropic API key format (sk-ant-*)
  - `formatMessagesForAnthropic()`: Converts messages to Anthropic's required format
  - `ensureAlternatingRoles()`: Ensures user/assistant message alternation
  - `createSuccessResponse()`: Maps Anthropic responses to standard format
  - `createErrorResponse()`: Maps Anthropic errors to standard failure format

## Architecture & Design
- Implements the Adapter pattern to abstract Anthropic-specific API details
- Handles Claude's unique requirements:
  - System messages as separate parameter (not in message array)
  - Messages must start with 'user' role
  - Strict alternating user/assistant message pattern
- Includes comprehensive error mapping for Anthropic-specific error cases
- Supports custom base URLs for Anthropic-compatible APIs

## Dependencies
- **Internal**: 
  - `../types`: LLMResponse, LLMFailureResponse, LLMMessage types
  - `./types`: ILLMClientAdapter, InternalLLMChatRequest, AdapterErrorCode
  - `./adapterErrorUtils`: Common error mapping utilities
- **External**: 
  - `@anthropic-ai/sdk`: Official Anthropic SDK for API communication

## Integration Points
- Used by LLMService when 'anthropic' provider is selected
- Registered in the adapter factory/configuration system
- Works with the unified error handling system via adapterErrorUtils
- Supports all Claude models configured in the system

## Usage Examples
```typescript
// Creating and using the adapter
const adapter = new AnthropicClientAdapter({ baseURL: 'custom-url' });
const response = await adapter.sendMessage({
  providerId: 'anthropic',
  modelId: 'claude-3-opus-20240229',
  messages: [{ role: 'user', content: 'Hello Claude!' }],
  settings: {
    maxTokens: 1000,
    temperature: 0.7,
    topP: 1.0,
    stopSequences: []
  }
}, apiKey);
```

## See Also
- `src/llm/LLMService.ts` - Main service that uses this adapter
- `src/llm/config.ts` - Configuration for Anthropic models
- `src/llm/clients/types.ts` - ILLMClientAdapter interface definition
- `src/llm/clients/adapterErrorUtils.ts` - Shared error handling utilities

### GeminiClientAdapter.ts
## Overview
The GeminiClientAdapter class implements the ILLMClientAdapter interface to provide integration with Google's Gemini generative AI APIs. It handles the translation between the library's standardized request/response format and Gemini's specific API requirements, including proper formatting of messages, safety settings, system instructions, and thinking/reasoning mode configuration.

## Key Components
- **GeminiClientAdapter class**: Main adapter implementing ILLMClientAdapter interface
  - `sendMessage()`: Core method that sends requests to Gemini API and returns standardized responses
  - `validateApiKey()`: Validates Gemini API key format (expects keys starting with 'AIza')
  - `getAdapterInfo()`: Returns adapter metadata
  - `formatInternalRequestToGemini()`: Converts internal request format to Gemini's expected structure, including thinking configuration
  - `createSuccessResponse()`: Maps Gemini responses to standardized LLMResponse format, extracting thought summaries when available
  - `createErrorResponse()`: Maps Gemini errors to standardized LLMFailureResponse format
  - `mapGeminiFinishReason()`: Translates Gemini-specific finish reasons to standard ones

## Architecture & Design
- Implements the Adapter pattern to abstract away Gemini-specific API details
- Handles Gemini's unique conversation format where assistant messages use "model" role instead of "assistant"
- Manages system instructions separately from the message flow (Gemini's preferred approach)
- Converts unified reasoning settings to Gemini's thinkingConfig with includeThoughts flag
- Extracts thought summaries from response parts marked with thought: true
- Provides comprehensive error mapping including Gemini-specific errors like safety filters
- Includes detailed logging for debugging API interactions

## Dependencies
- **Internal**: 
  - `../types` - LLMResponse, LLMFailureResponse, GeminiSafetySetting types
  - `./types` - ILLMClientAdapter interface, InternalLLMChatRequest, error codes
  - `./adapterErrorUtils` - Common error mapping utilities
- **External**: 
  - `@google/genai` - Google's official Gemini SDK

## Integration Points
- Consumed by `LLMService` as one of the available provider adapters
- Registered in the adapter configuration system for dynamic instantiation
- Works with the unified request/response types defined in the library's type system
- Integrates with the common error handling patterns via `adapterErrorUtils`

## Usage Examples
```typescript
// Typical usage within LLMService
const adapter = new GeminiClientAdapter();
const response = await adapter.sendMessage({
  providerId: 'gemini',
  modelId: 'gemini-pro',
  messages: [{ role: 'user', content: 'Hello!' }],
  settings: {
    maxTokens: 1000,
    temperature: 0.7,
    geminiSafetySettings: [/* safety config */]
  }
}, apiKey);
```

## See Also
- `src/llm/LLMService.ts` - Main service that uses this adapter
- `src/llm/clients/types.ts` - Interface definitions this adapter implements
- `src/llm/types.ts` - Type definitions for requests and responses
- `src/llm/config.ts` - Configuration where Gemini models are registered

### MockClientAdapter.ts
## Overview
The MockClientAdapter is a testing implementation of the ILLMClientAdapter interface that simulates various LLM provider behaviors without making actual API calls. It provides deterministic responses based on request content patterns and can simulate both successful responses and various error conditions, making it ideal for development, testing, and debugging LLM integrations.

## Key Components
- **MockClientAdapter class**: Main class implementing ILLMClientAdapter interface
- **sendMessage()**: Simulates LLM API calls with configurable delays and responses
- **validateApiKey()**: Basic validation that always returns true for non-empty keys
- **getAdapterInfo()**: Returns mock adapter metadata
- **createSuccessResponse()**: Generates mock successful LLM responses with token counting
- **createErrorResponse()**: Creates standardized error responses for various failure scenarios
- **simulateDelay()**: Adds realistic network latency simulation (100-500ms)

## Architecture & Design
- Implements pattern matching on message content to trigger specific behaviors
- Simulates various error conditions (rate limits, invalid keys, context length, etc.)
- Provides temperature-based response variations to mimic LLM creativity settings
- Includes token counting estimation (roughly 4 characters per token)
- Respects LLM settings like maxTokens, stopSequences, and temperature
- Uses deterministic patterns for reproducible testing scenarios

## Dependencies
- **Internal**: 
  - `../types` - LLM response types and settings
  - `./types` - Adapter interface and error codes
- **External**: None (pure TypeScript implementation)

## Integration Points
- Plugs into the LLMService as a drop-in replacement for real API adapters
- Compatible with the same ILLMClientAdapter interface used by real providers
- Can be configured with any providerId to simulate different providers
- Works with the unified error handling system via AdapterErrorCode

## Usage Examples
```typescript
// Create mock adapter
const mockAdapter = new MockClientAdapter("openai");

// Test error handling by including error keywords
const errorResponse = await mockAdapter.sendMessage({
  messages: [{ role: "user", content: "error_rate_limit" }],
  // ... other request params
}, "mock-api-key");

// Test temperature effects
const creativeResponse = await mockAdapter.sendMessage({
  messages: [{ role: "user", content: "test_temperature" }],
  settings: { temperature: 0.9, /* ... */ }
}, "mock-api-key");
```

## See Also
- `ILLMClientAdapter` interface in `./types.ts` - Interface this mock implements
- `LLMService.ts` - Main service that uses this adapter
- Other client adapters (OpenAI, Anthropic, etc.) - Real implementations this mocks
- `adapterErrorUtils.ts` - Error handling utilities used by all adapters

### Test Files
The directory includes comprehensive test coverage for all adapters and utilities:
- **adapterErrorUtils.test.ts**: Tests error mapping functionality
- **OpenAIClientAdapter.test.ts**: Tests OpenAI adapter implementation
- **AnthropicClientAdapter.test.ts**: Tests Anthropic adapter implementation
- **GeminiClientAdapter.test.ts**: Tests Gemini adapter implementation
- **MockClientAdapter.test.ts**: Tests mock adapter behavior

## Architecture
The clients directory implements a robust Adapter pattern architecture:
1. **Interface Definition** (types.ts): Defines the contract all adapters must implement
2. **Concrete Adapters**: Provider-specific implementations (OpenAI, Anthropic, Gemini)
3. **Mock Adapter**: Testing implementation for development and unit tests
4. **Shared Utilities** (adapterErrorUtils): Common error handling logic
5. **Comprehensive Testing**: Full test coverage for all adapters

This architecture enables:
- Easy addition of new AI providers by implementing ILLMClientAdapter
- Consistent error handling across all providers
- Provider-specific optimizations while maintaining a unified interface
- Reliable testing without requiring actual API calls

## Internal Dependencies
- All adapters depend on the types.ts interface definition
- Adapters use adapterErrorUtils for standardized error mapping
- Adapters import types from parent ../types module
- Test files depend on their respective implementation files

## External Dependencies
- `openai` - OpenAI Node.js SDK (for OpenAIClientAdapter)
- `@anthropic-ai/sdk` - Anthropic SDK (for AnthropicClientAdapter)
- `@google/genai` - Google Gemini SDK (for GeminiClientAdapter)
- Jest - Testing framework (for all test files)

## Integration Points
The clients directory serves as the provider integration layer:
- **Upward**: All adapters are consumed by LLMService
- **Configuration**: Adapters are registered in src/llm/config.ts
- **Type System**: Implements interfaces from types.ts and ../types.ts
- **Error Handling**: Uses standardized error codes for consistent error reporting

## Usage Examples
```typescript
// Example of how adapters are used by LLMService
import { OpenAIClientAdapter } from './clients/OpenAIClientAdapter';
import { AnthropicClientAdapter } from './clients/AnthropicClientAdapter';

// Adapter registration in config
const ADAPTER_CONSTRUCTORS = {
  openai: OpenAIClientAdapter,
  anthropic: AnthropicClientAdapter,
  // ... other adapters
};

// Dynamic adapter instantiation
const AdapterClass = ADAPTER_CONSTRUCTORS[providerId];
const adapter = new AdapterClass(config);

// Unified API call regardless of provider
const response = await adapter.sendMessage(request, apiKey);
```

## See Also
- **Parent Directory**: src/llm/ - Contains LLMService and configuration
- **Related Directories**:
  - src/llm/ - Main service that orchestrates adapter usage
  - src/types.ts - Core type definitions
- **Key Consumers**: LLMService uses these adapters for all provider interactions