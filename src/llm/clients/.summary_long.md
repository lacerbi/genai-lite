# Directory: src/llm/clients/

## Overview
The clients directory implements the Adapter pattern for integrating multiple AI provider APIs into a unified interface. This directory contains provider-specific client adapters (OpenAI, Anthropic, Google Gemini, Mistral, OpenRouter, llama.cpp) that translate between the library's standardized request/response format and each provider's unique API requirements. It also includes comprehensive type definitions and a mock adapter for testing. Each adapter handles authentication, request formatting, API communication, response normalization, and error mapping to ensure consistent behavior across all providers. The llama.cpp integration uses a hybrid architecture with a dedicated server client for utility operations alongside the standard adapter pattern.

**Note**: Shared error handling utilities have been moved to `src/shared/adapters/errorUtils.ts` as part of Phase 3.5 refactoring.

## Key Components
### types.ts
## Overview
This file establishes the core interface contract for LLM client adapters in the genai-lite library. It defines the standardized interface that all AI provider adapters (OpenAI, Anthropic, Google, Mistral, etc.) must implement to ensure consistent behavior across different providers. The file also includes helper types and error code constants to maintain consistency in error handling across adapters.

## Key Components
- **InternalLLMChatRequest**: Extended interface that ensures all settings have required values, removing the need for adapters to handle undefined values
- **ILLMClientAdapter**: Main interface defining the contract with methods:
  - `sendMessage()`: Core method for sending chat requests to LLM providers
  - `validateApiKey()`: Optional method for API key format validation
  - `getAdapterInfo()`: Optional method for provider metadata
- **ADAPTER_ERROR_CODES**: Constant object defining standardized error codes for consistent error handling
- **AdapterErrorCode**: Type helper derived from the error codes constant

## Architecture & Design
The design follows the Adapter pattern, allowing the library to support multiple AI providers through a common interface. Key architectural decisions include:
- Using TypeScript's `Required<T>` utility type to ensure settings are fully populated before reaching adapters
- Making error handling methods optional to allow flexibility while encouraging consistency
- Returning either success or failure responses rather than throwing errors, promoting predictable error handling
- Providing standardized error codes to ensure consistent error categorization across providers

## Dependencies
- **Internal**: 
  - `../types` - Core LLM types including LLMChatRequest, LLMResponse, LLMFailureResponse, and LLMSettings
- **External**: None (pure TypeScript type definitions)

## Integration Points
This interface is implemented by all provider-specific adapters:
- `OpenAIClientAdapter.ts`
- `AnthropicClientAdapter.ts`
- `GeminiClientAdapter.ts`
- `MistralClientAdapter.ts`

The LLMService uses this interface to interact with adapters in a provider-agnostic way.

## Usage Examples
```typescript
// Example adapter implementation
class OpenAIClientAdapter implements ILLMClientAdapter {
  async sendMessage(
    request: InternalLLMChatRequest,
    apiKey: string
  ): Promise<LLMResponse | LLMFailureResponse> {
    try {
      // Provider-specific API call
      const result = await openai.chat.completions.create({...});
      return { success: true, content: result.choices[0].message.content };
    } catch (error) {
      return {
        success: false,
        error: error.message,
        errorCode: ADAPTER_ERROR_CODES.PROVIDER_ERROR
      };
    }
  }
}
```

## See Also
- `src/llm/clients/OpenAIClientAdapter.ts` - Example implementation
- `src/llm/LLMService.ts` - Main service that uses these adapters
- `src/llm/types.ts` - Core types used by this interface
- `src/shared/adapters/errorUtils.ts` - Shared error handling utilities for adapters

### OpenAIClientAdapter.ts
## Overview
The OpenAIClientAdapter implements the ILLMClientAdapter interface to provide integration with OpenAI's chat completions API. It handles request formatting, authentication, API communication, and response normalization, converting between the application's internal format and OpenAI's specific API requirements.

## Key Components
- **OpenAIClientAdapter class**: Main adapter implementing ILLMClientAdapter interface
  - `sendMessage()`: Sends chat requests to OpenAI API and returns standardized responses
  - `validateApiKey()`: Validates OpenAI API key format (must start with 'sk-' and be ≥20 chars)
  - `getAdapterInfo()`: Returns adapter metadata
- **Private methods**:
  - `formatMessages()`: Converts internal message format to OpenAI's expected format
  - `createSuccessResponse()`: Transforms OpenAI completion to standardized LLMResponse
  - `createErrorResponse()`: Maps OpenAI errors to standardized LLMFailureResponse

## Architecture & Design
- Implements adapter pattern for provider abstraction
- Uses OpenAI's official Node.js client library
- Supports custom base URLs for OpenAI-compatible APIs
- Comprehensive error handling with specific error code mapping
- Logging for debugging API calls and responses

## Dependencies
- **Internal**: 
  - `../types` (LLMResponse, LLMFailureResponse types)
  - `./types` (ILLMClientAdapter, InternalLLMChatRequest, AdapterErrorCode)
  - `../../shared/adapters/errorUtils` (getCommonMappedErrorDetails utility)
- **External**: 
  - `openai` - Official OpenAI Node.js client library

## Integration Points
- Implements standardized ILLMClientAdapter interface for use by LLMService
- Converts between internal message format and OpenAI's chat completion format
- Maps OpenAI-specific errors to standardized error codes (e.g., CONTEXT_LENGTH_EXCEEDED, CONTENT_FILTER)
- Supports all OpenAI chat parameters: temperature, max_tokens, top_p, stop sequences, penalties

## Usage Examples
```typescript
// Create adapter with custom base URL
const adapter = new OpenAIClientAdapter({ 
  baseURL: 'https://custom-openai-api.com' 
});

// Send message through adapter
const response = await adapter.sendMessage({
  providerId: 'openai',
  modelId: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello!' }],
  settings: {
    temperature: 0.7,
    maxTokens: 1000,
    // ... other settings
  }
}, apiKey);
```

## See Also
- `../LLMService.ts` - Main service that uses this adapter
- `./types.ts` - Interface definitions and error codes
- `./adapterErrorUtils.ts` - Shared error mapping utilities
- Other adapters: AnthropicClientAdapter, GeminiClientAdapter, MistralClientAdapter

### AnthropicClientAdapter.ts
## Overview
The AnthropicClientAdapter implements the ILLMClientAdapter interface to provide integration with Anthropic's Claude models. It handles the specific requirements of Anthropic's Messages API, including proper message formatting, system message positioning, and the conversion between standardized LLM formats and Anthropic's proprietary API structure.

## Key Components
- **AnthropicClientAdapter class**: Main adapter implementing ILLMClientAdapter
  - `sendMessage()`: Sends chat completion requests to Anthropic's API
  - `validateApiKey()`: Validates Anthropic API key format (sk-ant-*)
  - `formatMessagesForAnthropic()`: Converts messages to Anthropic's required format
  - `ensureAlternatingRoles()`: Ensures user/assistant message alternation
  - `createSuccessResponse()`: Maps Anthropic responses to standard format
  - `createErrorResponse()`: Maps Anthropic errors to standard failure format

## Architecture & Design
- Implements the Adapter pattern to abstract Anthropic-specific API details
- Handles Claude's unique requirements:
  - System messages as separate parameter (not in message array)
  - Messages must start with 'user' role
  - Strict alternating user/assistant message pattern
- Includes comprehensive error mapping for Anthropic-specific error cases
- Supports custom base URLs for Anthropic-compatible APIs

## Dependencies
- **Internal**: 
  - `../types`: LLMResponse, LLMFailureResponse, LLMMessage types
  - `./types`: ILLMClientAdapter, InternalLLMChatRequest, AdapterErrorCode
  - `../../shared/adapters/errorUtils`: Common error mapping utilities
- **External**: 
  - `@anthropic-ai/sdk`: Official Anthropic SDK for API communication

## Integration Points
- Used by LLMService when 'anthropic' provider is selected
- Registered in the adapter factory/configuration system
- Works with the unified error handling system via adapterErrorUtils
- Supports all Claude models configured in the system

## Usage Examples
```typescript
// Creating and using the adapter
const adapter = new AnthropicClientAdapter({ baseURL: 'custom-url' });
const response = await adapter.sendMessage({
  providerId: 'anthropic',
  modelId: 'claude-3-opus-20240229',
  messages: [{ role: 'user', content: 'Hello Claude!' }],
  settings: {
    maxTokens: 1000,
    temperature: 0.7,
    topP: 1.0,
    stopSequences: []
  }
}, apiKey);
```

## See Also
- `src/llm/LLMService.ts` - Main service that uses this adapter
- `src/llm/config.ts` - Configuration for Anthropic models
- `src/llm/clients/types.ts` - ILLMClientAdapter interface definition
- `src/llm/clients/adapterErrorUtils.ts` - Shared error handling utilities

### GeminiClientAdapter.ts
## Overview
The GeminiClientAdapter class implements the ILLMClientAdapter interface to provide integration with Google's Gemini generative AI APIs. It handles the translation between the library's standardized request/response format and Gemini's specific API requirements, including proper formatting of messages, safety settings, system instructions, and thinking/reasoning mode configuration.

## Key Components
- **GeminiClientAdapter class**: Main adapter implementing ILLMClientAdapter interface
  - `sendMessage()`: Core method that sends requests to Gemini API and returns standardized responses
  - `validateApiKey()`: Validates Gemini API key format (expects keys starting with 'AIza')
  - `getAdapterInfo()`: Returns adapter metadata
  - `formatInternalRequestToGemini()`: Converts internal request format to Gemini's expected structure, including thinking configuration
  - `createSuccessResponse()`: Maps Gemini responses to standardized LLMResponse format, extracting thought summaries when available
  - `createErrorResponse()`: Maps Gemini errors to standardized LLMFailureResponse format
  - `mapGeminiFinishReason()`: Translates Gemini-specific finish reasons to standard ones

## Architecture & Design
- Implements the Adapter pattern to abstract away Gemini-specific API details
- Handles Gemini's unique conversation format where assistant messages use "model" role instead of "assistant"
- Manages system instructions separately from the message flow (Gemini's preferred approach)
- Converts unified reasoning settings to Gemini's thinkingConfig with includeThoughts flag
- Extracts thought summaries from response parts marked with thought: true
- Provides comprehensive error mapping including Gemini-specific errors like safety filters
- Includes detailed logging for debugging API interactions

## Dependencies
- **Internal**: 
  - `../types` - LLMResponse, LLMFailureResponse, GeminiSafetySetting types
  - `./types` - ILLMClientAdapter interface, InternalLLMChatRequest, error codes
  - `../../shared/adapters/errorUtils` - Common error mapping utilities
- **External**: 
  - `@google/genai` - Google's official Gemini SDK

## Integration Points
- Consumed by `LLMService` as one of the available provider adapters
- Registered in the adapter configuration system for dynamic instantiation
- Works with the unified request/response types defined in the library's type system
- Integrates with the common error handling patterns via `adapterErrorUtils`

## Usage Examples
```typescript
// Typical usage within LLMService
const adapter = new GeminiClientAdapter();
const response = await adapter.sendMessage({
  providerId: 'gemini',
  modelId: 'gemini-pro',
  messages: [{ role: 'user', content: 'Hello!' }],
  settings: {
    maxTokens: 1000,
    temperature: 0.7,
    geminiSafetySettings: [/* safety config */]
  }
}, apiKey);
```

## See Also
- `src/llm/LLMService.ts` - Main service that uses this adapter
- `src/llm/clients/types.ts` - Interface definitions this adapter implements
- `src/llm/types.ts` - Type definitions for requests and responses
- `src/llm/config.ts` - Configuration where Gemini models are registered

### MockClientAdapter.ts
## Overview
The MockClientAdapter is a testing implementation of the ILLMClientAdapter interface that simulates various LLM provider behaviors without making actual API calls. It provides deterministic responses based on request content patterns and can simulate both successful responses and various error conditions, making it ideal for development, testing, and debugging LLM integrations.

## Key Components
- **MockClientAdapter class**: Main class implementing ILLMClientAdapter interface
- **sendMessage()**: Simulates LLM API calls with configurable delays and responses
- **validateApiKey()**: Basic validation that always returns true for non-empty keys
- **getAdapterInfo()**: Returns mock adapter metadata
- **createSuccessResponse()**: Generates mock successful LLM responses with token counting
- **createErrorResponse()**: Creates standardized error responses for various failure scenarios
- **simulateDelay()**: Adds realistic network latency simulation (100-500ms)

## Architecture & Design
- Implements pattern matching on message content to trigger specific behaviors
- Simulates various error conditions (rate limits, invalid keys, context length, etc.)
- Provides temperature-based response variations to mimic LLM creativity settings
- Includes token counting estimation (roughly 4 characters per token)
- Respects LLM settings like maxTokens, stopSequences, and temperature
- Uses deterministic patterns for reproducible testing scenarios

## Dependencies
- **Internal**: 
  - `../types` - LLM response types and settings
  - `./types` - Adapter interface and error codes
- **External**: None (pure TypeScript implementation)

## Integration Points
- Plugs into the LLMService as a drop-in replacement for real API adapters
- Compatible with the same ILLMClientAdapter interface used by real providers
- Can be configured with any providerId to simulate different providers
- Works with the unified error handling system via AdapterErrorCode

## Usage Examples
```typescript
// Create mock adapter
const mockAdapter = new MockClientAdapter("openai");

// Test error handling by including error keywords
const errorResponse = await mockAdapter.sendMessage({
  messages: [{ role: "user", content: "error_rate_limit" }],
  // ... other request params
}, "mock-api-key");

// Test temperature effects
const creativeResponse = await mockAdapter.sendMessage({
  messages: [{ role: "user", content: "test_temperature" }],
  settings: { temperature: 0.9, /* ... */ }
}, "mock-api-key");
```

## See Also
- `ILLMClientAdapter` interface in `./types.ts` - Interface this mock implements
- `LLMService.ts` - Main service that uses this adapter
- Other client adapters (OpenAI, Anthropic, etc.) - Real implementations this mocks
- `adapterErrorUtils.ts` - Error handling utilities used by all adapters

### LlamaCppServerClient.ts
## Overview
LlamaCppServerClient provides a comprehensive utility class for interacting with llama.cpp server's management and utility endpoints (non-LLM features). This client enables tokenization, embedding generation, health monitoring, and server management operations for locally-running llama.cpp instances.

## Key Components
- **LlamaCppServerClient class**: Main utility class for server endpoints
  - `getHealth()`: Checks server readiness status (`/health`)
  - `tokenize()`: Converts text to token IDs using the loaded model's tokenizer
  - `detokenize()`: Converts token IDs back to text
  - `createEmbedding()`: Generates embedding vectors for text (supports multimodal with images)
  - `infill()`: Performs code completion between prefix and suffix (for FIM models)
  - `getProps()`: Retrieves server properties and configuration
  - `getMetrics()`: Fetches performance metrics (Prometheus or JSON format)
  - `getSlots()`: Accesses processing slot status (debugging endpoint with security warning)

## Architecture & Design
- Separates server utility operations from chat completions (see LlamaCppClientAdapter for chat)
- Uses native fetch API for HTTP requests (no external HTTP client dependency)
- Comprehensive TypeScript interfaces for all response types
- Includes security warnings for sensitive endpoints (e.g., `/slots` exposes prompt content)
- Handles both JSON and Prometheus-format responses from `/metrics`

## Dependencies
- **Internal**: None (standalone utility class)
- **External**: None (uses native fetch)

## Integration Points
- Used by LlamaCppClientAdapter for optional health checks before chat requests
- Can be accessed directly via `LlamaCppClientAdapter.getServerClient()` method
- Exported from main index.ts for standalone usage

## Usage Examples
```typescript
// Create client for local server
const client = new LlamaCppServerClient('http://localhost:8080');

// Check server health
const health = await client.getHealth();
if (health.status === 'ok') {
  console.log('Server ready');
}

// Tokenize text
const { tokens } = await client.tokenize('Hello world');
console.log(`Text has ${tokens.length} tokens`);

// Generate embeddings
const { embedding } = await client.createEmbedding('Search query');
console.log(`Embedding dimension: ${embedding.length}`);

// Code completion (FIM models)
const result = await client.infill('function add(a, b) {\n  ', '\n}');
console.log(result.content); // 'return a + b;'
```

## See Also
- `./LlamaCppClientAdapter.ts` - Chat completion adapter for llama.cpp
- `../LLMService.ts` - Main service that can use llama.cpp
- `../config.ts` - llama.cpp provider registration

### LlamaCppClientAdapter.ts
## Overview
LlamaCppClientAdapter implements the ILLMClientAdapter interface to provide chat completion integration with llama.cpp server. It uses llama.cpp's OpenAI-compatible API endpoint (`/v1/chat/completions`) by leveraging the OpenAI SDK with a custom base URL. This adapter is the sixth provider integration in genai-lite, alongside OpenAI, Anthropic, Gemini, Mistral, and OpenRouter.

## Key Components
- **LlamaCppClientAdapter class**: Main adapter implementing ILLMClientAdapter interface
  - `sendMessage()`: Sends chat requests to llama.cpp's OpenAI-compatible endpoint
  - `validateApiKey()`: Always returns true (llama.cpp doesn't require API keys)
  - `getAdapterInfo()`: Returns adapter metadata
  - `getServerClient()`: Provides access to LlamaCppServerClient for utility operations
- **LlamaCppClientConfig interface**: Configuration options
  - `baseURL`: Server URL (default: http://localhost:8080)
  - `checkHealth`: Whether to verify server health before requests (default: false)

## Architecture & Design
- **Hybrid Architecture**: Uses two complementary classes
  - LlamaCppClientAdapter: For LLM chat completions (unified interface)
  - LlamaCppServerClient: For server utilities (tokenization, embeddings, etc.)
- Uses OpenAI SDK internally with custom baseURL pointing to llama.cpp
- No API key required (local server) - passes 'not-needed' to SDK
- Optional health check before requests to catch server-not-ready errors early
- Comprehensive connection error handling with helpful messages

## Dependencies
- **Internal**:
  - `../types` - LLMResponse, LLMFailureResponse types
  - `./types` - ILLMClientAdapter interface, error codes
  - `../../shared/adapters/errorUtils` - Error mapping utilities
  - `./LlamaCppServerClient` - Server utility operations
- **External**:
  - `openai` - Official OpenAI SDK (for OpenAI-compatible API)

## Integration Points
- Implements ILLMClientAdapter for use by LLMService
- Registered in config.ts as 'llamacpp' provider
- Works with any GGUF model loaded in llama.cpp server
- Model IDs are not validated (users specify whatever model they loaded)
- Supports multiple simultaneous servers via different adapter instances

## Usage Examples
```typescript
// Basic usage with LLMService
const service = new LLMService(async () => 'not-needed');
const response = await service.sendMessage({
  providerId: 'llamacpp',
  modelId: 'llama-3-8b-instruct',
  messages: [{ role: 'user', content: 'Hello!' }]
});

// Custom configuration
const adapter = new LlamaCppClientAdapter({
  baseURL: 'http://localhost:8080',
  checkHealth: true  // Verify server ready before requests
});
service.registerAdapter('llamacpp', adapter);

// Multiple servers
service.registerAdapter('llamacpp-small',
  new LlamaCppClientAdapter({ baseURL: 'http://localhost:8080' }));
service.registerAdapter('llamacpp-large',
  new LlamaCppClientAdapter({ baseURL: 'http://localhost:8081' }));

// Access server utilities
const serverClient = adapter.getServerClient();
const { tokens } = await serverClient.tokenize('Hello world');
```

## See Also
- `./LlamaCppServerClient.ts` - Server utility endpoints (tokenization, embeddings, etc.)
- `../LLMService.ts` - Main service that uses this adapter
- `../config.ts` - Provider registration and configuration
- `./types.ts` - ILLMClientAdapter interface definition

### OpenRouterClientAdapter.ts
## Overview
The OpenRouterClientAdapter implements the ILLMClientAdapter interface to provide integration with OpenRouter, an API gateway that offers unified access to 100+ LLM models from various providers (OpenAI, Anthropic, Google, Meta, Mistral, etc.) through a single OpenAI-compatible API.

## Key Components
- **OpenRouterClientAdapter class**: Main adapter implementing ILLMClientAdapter interface
  - `sendMessage()`: Sends chat requests to OpenRouter API with optional provider routing
  - `validateApiKey()`: Validates OpenRouter API key format (must start with 'sk-or-' and be ≥40 chars)
  - `getAdapterInfo()`: Returns adapter metadata
- **OpenRouterClientConfig interface**: Configuration options
  - `baseURL`: API endpoint (default: https://openrouter.ai/api/v1)
  - `httpReferer`: App URL for rankings attribution (optional)
  - `siteTitle`: App display name for rankings (optional)
- **Provider Routing**: Supports OpenRouter-specific provider control settings
  - `order`: Preferred provider order
  - `ignore`: Providers to exclude
  - `allow`: Providers to allow
  - `dataCollection`: Training data opt-in/out
  - `requireParameters`: Enforce parameter requirements

## Architecture & Design
- Uses OpenAI SDK with custom baseURL pointing to OpenRouter's gateway
- Leverages shared system message utilities from `src/shared/adapters/systemMessageUtils`
- Supports app attribution via HTTP-Referer and X-Title headers (improves rankings)
- Model IDs use provider/model format (e.g., "openai/gpt-4", "anthropic/claude-3-opus", "google/gemma-3-27b-it:free")
- Comprehensive error handling with OpenRouter-specific error refinements

## Dependencies
- **Internal**:
  - `../types` - LLMResponse, LLMFailureResponse types
  - `./types` - ILLMClientAdapter interface, InternalLLMChatRequest, error codes
  - `../../shared/adapters/errorUtils` - Common error mapping utilities
  - `../../shared/adapters/systemMessageUtils` - System message handling
  - `../../logging/defaultLogger` - Logging functionality
- **External**:
  - `openai` - Official OpenAI SDK (for OpenAI-compatible API)

## Integration Points
- Implements ILLMClientAdapter for use by LLMService
- Registered in config.ts as 'openrouter' provider
- Uses shared error mapping from errorUtils
- Supports all OpenAI-compatible parameters

## See Also
- `../LLMService.ts` - Main service that uses this adapter
- `./types.ts` - Interface definitions and error codes
- Other adapters: OpenAIClientAdapter, AnthropicClientAdapter, GeminiClientAdapter, LlamaCppClientAdapter

### Test Files
The directory includes comprehensive test coverage for all adapters:
- **OpenAIClientAdapter.test.ts**: Tests OpenAI adapter implementation
- **AnthropicClientAdapter.test.ts**: Tests Anthropic adapter implementation
- **GeminiClientAdapter.test.ts**: Tests Gemini adapter implementation
- **OpenRouterClientAdapter.test.ts**: Tests OpenRouter adapter including API key validation, provider routing, and error handling
- **LlamaCppServerClient.test.ts**: Tests llama.cpp server utility endpoints (25 tests, 100% coverage)
- **LlamaCppClientAdapter.test.ts**: Tests llama.cpp chat adapter (26 tests, 98.11% coverage)
- **MockClientAdapter.test.ts**: Tests mock adapter behavior

## Architecture
The clients directory implements a robust Adapter pattern architecture:
1. **Interface Definition** (types.ts): Defines the contract all adapters must implement
2. **Concrete Adapters**: Provider-specific implementations (OpenAI, Anthropic, Gemini, Mistral, OpenRouter, llama.cpp)
3. **Hybrid llama.cpp Integration**:
   - LlamaCppClientAdapter for chat completions (implements ILLMClientAdapter)
   - LlamaCppServerClient for server utilities (standalone client)
4. **Mock Adapter**: Testing implementation for development and unit tests
5. **Shared Utilities** (src/shared/adapters/errorUtils): Common error handling logic
6. **Comprehensive Testing**: Full test coverage for all adapters

This architecture enables:
- Easy addition of new AI providers by implementing ILLMClientAdapter
- Consistent error handling across all providers
- Provider-specific optimizations while maintaining a unified interface
- Reliable testing without requiring actual API calls
- Support for both cloud and local LLM providers

## Internal Dependencies
- All adapters depend on the types.ts interface definition
- Adapters use `src/shared/adapters/errorUtils` for standardized error mapping
- Adapters import types from parent ../types module
- LlamaCppClientAdapter depends on LlamaCppServerClient for utility operations
- Test files depend on their respective implementation files

## External Dependencies
- `openai` - OpenAI Node.js SDK (for OpenAIClientAdapter and LlamaCppClientAdapter)
- `@anthropic-ai/sdk` - Anthropic SDK (for AnthropicClientAdapter)
- `@google/genai` - Google Gemini SDK (for GeminiClientAdapter)
- Jest - Testing framework (for all test files)

## Integration Points
The clients directory serves as the provider integration layer:
- **Upward**: All adapters are consumed by LLMService
- **Configuration**: Adapters are registered in src/llm/config.ts
- **Type System**: Implements interfaces from types.ts and ../types.ts
- **Error Handling**: Uses standardized error codes for consistent error reporting
- **Local Inference**: llama.cpp adapter enables privacy-focused local LLM inference without API keys

## Usage Examples
```typescript
// Example of how adapters are used by LLMService
import { OpenAIClientAdapter } from './clients/OpenAIClientAdapter';
import { AnthropicClientAdapter } from './clients/AnthropicClientAdapter';
import { LlamaCppClientAdapter } from './clients/LlamaCppClientAdapter';

// Adapter registration in config
const ADAPTER_CONSTRUCTORS = {
  openai: OpenAIClientAdapter,
  anthropic: AnthropicClientAdapter,
  llamacpp: LlamaCppClientAdapter,
  // ... other adapters
};

// Dynamic adapter instantiation
const AdapterClass = ADAPTER_CONSTRUCTORS[providerId];
const adapter = new AdapterClass(config);

// Unified API call regardless of provider (cloud or local)
const response = await adapter.sendMessage(request, apiKey);

// llama.cpp-specific: Access server utilities
if (adapter instanceof LlamaCppClientAdapter) {
  const serverClient = adapter.getServerClient();
  const { tokens } = await serverClient.tokenize('Hello world');
  const { embedding } = await serverClient.createEmbedding('Some text');
}
```

## See Also
- **Parent Directory**: src/llm/ - Contains LLMService and configuration
- **Related Directories**:
  - src/llm/ - Main service that orchestrates adapter usage
  - src/types.ts - Core type definitions
- **Key Consumers**: LLMService uses these adapters for all provider interactions