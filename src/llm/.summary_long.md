# Directory: src/llm/

## Overview
The llm directory forms the core of the genai-lite library, housing the main LLM service implementation, comprehensive type definitions, and a sophisticated configuration system. This directory orchestrates interactions with multiple AI providers (OpenAI, Anthropic, Google Gemini, Mistral) through a unified interface while maintaining provider-specific customizations and capabilities. The architecture follows best practices with clear separation between service logic, configuration, and type definitions, all backed by thorough test coverage.

## Key Components
### LLMService.ts
## Overview
The LLMService class serves as the central orchestrator for all LLM operations in the genai-lite library. Following a successful refactoring from 872 lines to ~400 lines, it now delegates specific responsibilities to specialized service classes while maintaining the same public API. The service coordinates between PresetManager, AdapterRegistry, RequestValidator, SettingsManager, and ModelResolver to provide a unified interface for interacting with multiple AI providers.

## Key Components
- **LLMService** (main class): Orchestrates LLM operations by coordinating specialized services
  - `constructor(apiKeyProvider, options?)`: Initializes service and all sub-services
  - `getProviders()`: Returns list of supported AI providers
  - `getModels(providerId)`: Returns available models for a specific provider
  - `getPresets()`: Delegates to PresetManager for configured presets
  - `sendMessage(request)`: Main method orchestrating the request pipeline through all services, now includes automatic thinking extraction post-processing with intelligent enforcement via onMissing ('auto' mode checks native reasoning status) and preserves LLM responses in partialResponse field when validation errors occur
  - `createMessages(options)`: Unified prompt creation that combines template rendering, model context injection, role tag parsing, and returns settings extracted from template metadata
  - `getRegisteredAdapters()`: Delegates to AdapterRegistry for adapter information
  - `getProviderSummary()`: Delegates to AdapterRegistry for provider status
- **LLMServiceOptions**: Configuration interface for customizing presets
- **PresetMode**: Type defining preset integration strategies (re-exported from PresetManager)
- **CreateMessagesResult**: Return type for createMessages including messages, modelContext, and settings

## Architecture & Design
- **Service-Oriented Architecture**: Delegates responsibilities to focused service classes:
  - **PresetManager**: Handles preset loading, merging, and resolution
  - **AdapterRegistry**: Manages adapter initialization and retrieval
  - **RequestValidator**: Validates requests, messages, and settings
  - **SettingsManager**: Merges settings and filters unsupported parameters
  - **ModelResolver**: Resolves model info from presets or direct IDs
- **Dependency Injection**: Accepts an ApiKeyProvider function for flexible API key management
- **Orchestration Pattern**: LLMService coordinates the flow between services without implementing business logic
- **Error Handling**: Centralized error handling with standardized failure responses that preserve original LLM responses in partialResponse field for validation errors
- **Clean Separation**: Each service has a single responsibility, improving testability and maintainability

## Dependencies
- **Internal**: 
  - `../types` (ApiKeyProvider type)
  - `./types` (LLM request/response types)
  - `./clients/types` (adapter interfaces)
  - `./config` (provider/model configurations)
  - `../prompting/template` (template rendering)
  - `../prompting/parser` (extractInitialTaggedContent for thinking extraction, parseRoleTags for message creation, parseTemplateWithMetadata for metadata extraction)
  - `./services/*` (all extracted service classes)
  
- **External**: None directly (services handle their own dependencies)

## Integration Points
- **API Key Management**: Integrates with any ApiKeyProvider implementation for secure key retrieval
- **Client Adapters**: Connects to provider-specific adapters (OpenAI, Anthropic, Gemini, Mistral)
- **Configuration System**: Uses centralized config for provider/model definitions and validation
- **Error Handling**: Provides standardized error responses across all providers, preserving LLM responses in partialResponse field for validation errors
- **Post-Processing**: Automatically extracts thinking/reasoning blocks from responses when configured

## Usage Examples
```typescript
// Initialize with environment variable provider and default presets
const apiKeyProvider = async (providerId: string) => {
  return process.env[`${providerId.toUpperCase()}_API_KEY`];
};
const llmService = new LLMService(apiKeyProvider);

// Send a message using presets (new functionality)
const response = await llmService.sendMessage({
  presetId: 'anthropic-claude-3-7-sonnet-20250219-thinking',
  messages: [
    { role: 'user', content: 'Solve this complex problem step by step...' }
  ]
});

// Use unified prompt creation (new functionality)
const { messages, modelContext, settings } = await llmService.createMessages({
  template: `
    <META>
    {
      "settings": {
        "temperature": 0.8,
        "maxTokens": 2000
      }
    }
    </META>
    <SYSTEM>
      {{ thinking_enabled ? "Please think step-by-step." : "Please analyze carefully." }}
      {{ thinking_available && !thinking_enabled ? "(Note: This model supports reasoning mode)" : "" }}
    </SYSTEM>
    <USER>{{ question }}</USER>
  `,
  variables: { question: 'What is quantum computing?' },
  presetId: 'anthropic-claude-3-7-sonnet-20250219-thinking'
});

// Send the created messages with extracted settings
const response = await llmService.sendMessage({ 
  presetId: 'anthropic-claude-3-7-sonnet-20250219-thinking', 
  messages,
  settings // Includes temperature: 0.8, maxTokens: 2000 from template
});

// Traditional message sending (still supported)
const response = await llmService.sendMessage({
  providerId: 'openai',
  modelId: 'gpt-4',
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'Hello, how are you?' }
  ],
  settings: {
    temperature: 0.7,
    maxTokens: 1000
  }
});

// Initialize with custom presets (extend mode)
const serviceWithExtended = new LLMService(apiKeyProvider, {
  presets: [{ id: 'custom-gpt4', displayName: 'GPT-4 Custom', providerId: 'openai', modelId: 'gpt-4', settings: { temperature: 0.5 } }],
  presetMode: 'extend'
});

// Get available models for a provider
const models = await llmService.getModels('anthropic');
```

## See Also
- `src/llm/config.ts` - Provider and model configurations
- `src/llm/clients/*ClientAdapter.ts` - Provider-specific adapter implementations
- `src/providers/fromEnvironment.ts` - Built-in environment variable API key provider
- `src/index.ts` - Main library entry point that exports LLMService

### types.ts
## Overview
This file serves as the central type definition module for the LLM (Large Language Model) interaction system in genai-lite. It provides a comprehensive set of TypeScript interfaces and types that define the structure of all LLM-related data, from basic message formats to complex provider configurations and safety settings.

## Key Components
- **LLMMessage & LLMMessageRole**: Core message structure with role-based conversation modeling (user/assistant/system)
- **LLMChatRequest**: Main request interface combining provider ID, model ID, messages, and optional settings
- **LLMResponse & LLMFailureResponse**: Success and error response structures with detailed metadata (LLMFailureResponse now includes optional partialResponse field that preserves the LLM response when validation errors occur)
- **LLMSettings**: Comprehensive configuration options including temperature, maxTokens, penalties, and provider-specific settings
- **LLMReasoningSettings**: Configuration for thinking/reasoning modes with enabled, effort, maxTokens, and exclude options
- **LLMThinkingExtractionSettings**: Configuration for automatic extraction of thinking blocks from responses with customizable tag names and intelligent enforcement via onMissing property ('ignore', 'warn', 'error', 'auto')
- **ProviderInfo & ModelInfo**: Metadata interfaces for AI providers and their models
- **ModelReasoningCapabilities**: Defines reasoning support, token budgets, and output types for thinking models
- **GeminiSafetySetting**: Google Gemini-specific safety configuration with harm categories and thresholds
- **LLMUsage**: Token usage tracking for billing and quota management
- **LLM_IPC_CHANNELS**: Constants for Inter-Process Communication in Electron contexts

## Architecture & Design
- Uses discriminated unions for response types (success vs error) via the `object` field
- Implements provider-agnostic base types with optional provider-specific extensions
- Supports flexible safety configurations through optional Gemini-specific settings
- Designed for extensibility with optional fields for future provider requirements
- Clear separation between request, response, and configuration concerns

## Dependencies
- **Internal**: None - this is a pure type definition file
- **External**: None - uses only TypeScript built-in types

## Integration Points
This type module serves as the contract between:
- LLM service implementations and client adapters
- API request builders and response parsers
- Configuration systems and runtime validators
- IPC communication layers (for Electron-based consumers)

## Usage Examples
```typescript
// Creating an LLM chat request
const request: LLMChatRequest = {
  providerId: 'openai',
  modelId: 'gpt-4',
  messages: [
    { role: 'system', content: 'You are a helpful assistant' },
    { role: 'user', content: 'Hello!' }
  ],
  settings: {
    temperature: 0.7,
    maxTokens: 1000
  }
};

// Handling responses with type guards
function handleResponse(response: LLMResponse | LLMFailureResponse) {
  if (response.object === 'error') {
    console.error('LLM Error:', response.error.message);
    // For validation errors, the response may still be available
    if (response.partialResponse) {
      console.log('Generated content:', response.partialResponse.choices[0].message.content);
    }
  } else {
    console.log('Assistant:', response.choices[0].message.content);
  }
}
```

## See Also
- `src/llm/clients/types.ts` - Client adapter interfaces that use these types
- `src/llm/LLMService.ts` - Main service implementation consuming these types
- `src/llm/config.ts` - Provider and model configurations using ModelInfo/ProviderInfo
- Provider-specific adapters that implement request/response transformations

### config.ts
## Overview
The config.ts file serves as the central configuration registry for the genai-lite library, defining all supported LLM providers (OpenAI, Anthropic, Gemini, Mistral) and their models with detailed specifications. It manages adapter mappings, default settings, model capabilities, pricing information, and provides utility functions for querying and validating LLM configurations.

## Key Components
- **ADAPTER_CONSTRUCTORS**: Maps provider IDs to their client adapter class constructors
- **ADAPTER_CONFIGS**: Optional configuration objects for each adapter (e.g., custom base URLs)
- **DEFAULT_LLM_SETTINGS**: Global default settings for all LLM requests (thinkingExtraction now defaults to enabled: false with onMissing: 'auto')
- **PROVIDER_DEFAULT_SETTINGS**: Provider-specific setting overrides
- **MODEL_DEFAULT_SETTINGS**: Model-specific setting overrides
- **SUPPORTED_PROVIDERS**: List of all provider configurations
- **SUPPORTED_MODELS**: Comprehensive list of all models with detailed specifications
- **Utility Functions**: getProviderById, getModelById, getModelsByProvider, isProviderSupported, isModelSupported, getDefaultSettingsForModel, validateLLMSettings

## Architecture & Design
- Uses a hierarchical configuration system with three levels of settings: global defaults → provider defaults → model defaults
- Model definitions include comprehensive metadata: pricing, context windows, capabilities, reasoning support, and limitations
- Supports dynamic adapter registration through the constructor mapping pattern
- Implements thorough validation for all LLM settings with specific range checks
- Includes special handling for provider-specific features (e.g., Gemini safety settings, reasoning capabilities)

## Dependencies
- **Internal**: 
  - `./types` - Type definitions for LLM configurations
  - `./clients/types` - ILLMClientAdapter interface
  - `./clients/OpenAIClientAdapter` - OpenAI implementation
  - `./clients/AnthropicClientAdapter` - Anthropic implementation
  - `./clients/GeminiClientAdapter` - Google Gemini implementation
- **External**: None (pure TypeScript configuration)

## Integration Points
- LLMService uses this config to instantiate appropriate adapters via ADAPTER_CONSTRUCTORS
- Client adapters reference model configurations for capability checks
- Validation functions are used by LLMService to ensure valid request parameters
- Environment variables can override adapter base URLs through ADAPTER_CONFIGS

## Usage Examples
```typescript
// Get model information
const model = getModelById('claude-3-5-sonnet-20241022', 'anthropic');
console.log(model.contextWindow); // 200000

// Get all models for a provider
const openaiModels = getModelsByProvider('openai');

// Get merged settings for a specific model
const settings = getDefaultSettingsForModel('o4-mini', 'openai');
// Returns settings with temperature: 1.0 (model override)

// Validate LLM settings
const errors = validateLLMSettings({ temperature: 3.0 });
// Returns ['temperature must be a number between 0 and 2']
```

## See Also
- `src/llm/LLMService.ts` - Main service that consumes this configuration
- `src/llm/clients/*ClientAdapter.ts` - Adapter implementations registered here
- `src/llm/types.ts` - Type definitions used throughout the configuration

### LLMService.test.ts
## Overview
Comprehensive test suite for the LLMService class, which serves as the main entry point for the genai-lite library. The tests verify proper initialization, request validation, error handling, API key management, adapter routing, settings validation, and provider/model information retrieval functionality.

## Test Coverage
- **Constructor and initialization**: Service creation and lazy-loading of client adapters
- **sendMessage validation**: Unsupported providers/models, empty messages, invalid roles, empty content
- **API key handling**: Null keys, provider errors, invalid key formats
- **Adapter routing**: Correct adapter selection, adapter reuse
- **Settings management**: Default settings, user setting merging, temperature/maxTokens/topP validation
- **Provider/model queries**: getProviders() and getModels() functionality

## Testing Approach
The tests use Jest with TypeScript, employing mocked API key providers to isolate the service logic. The test suite uses a clean setup/teardown pattern with `beforeEach` to ensure test isolation. Mock functions are used extensively to simulate API key retrieval without requiring actual API credentials. The tests focus on edge cases and error conditions rather than successful API calls, as network requests are not mocked.

## Dependencies
- **Internal**: LLMService, ApiKeyProvider type, LLM types (LLMChatRequest, LLMResponse, LLMFailureResponse), ADAPTER_ERROR_CODES
- **External**: Jest testing framework with TypeScript support

## Key Test Scenarios
- Validation of all request parameters (provider, model, messages)
- Proper error codes and messages for various failure scenarios
- API key provider error handling (null returns, exceptions)
- Settings validation with boundary testing (temperature ranges, token limits)
- Provider and model enumeration functionality
- Lazy-loading behavior of client adapters

## Usage Examples
```typescript
// Running the tests
npm test src/llm/LLMService.test.ts

// Example of extending tests for a new provider
it('should support new provider', async () => {
  const request: LLMChatRequest = {
    providerId: 'new-provider',
    modelId: 'new-model',
    messages: [{ role: 'user', content: 'Test' }]
  };
  const response = await service.sendMessage(request);
  // Assert expected behavior
});
```

## See Also
- LLMService.ts - The implementation file being tested
- types.ts - Type definitions for ApiKeyProvider
- llm/types.ts - LLM-specific type definitions
- llm/config.ts - Provider and model configurations

### config.test.ts
## Overview
This test file provides comprehensive unit tests for the LLM configuration module (`config.ts`), which manages AI provider and model configurations. It validates all exported functions including provider/model lookup, settings retrieval, and parameter validation to ensure the configuration system works correctly across all supported LLM providers.

## Test Coverage
- **Provider Management**: Tests for `isProviderSupported`, `getProviderById`
- **Model Management**: Tests for `getModelById`, `getModelsByProvider`, `isModelSupported`
- **Settings Management**: Tests for `getDefaultSettingsForModel`, `validateLLMSettings`
- **Edge Cases**: Invalid inputs, unsupported providers/models, boundary value testing

## Testing Approach
The tests use Jest as the testing framework with a describe/it structure for clear organization. Each function is tested in isolation with both positive and negative test cases. No mocking is used as the config module contains pure functions without external dependencies. Tests focus on verifying correct return values, type safety, and proper handling of invalid inputs.

## Dependencies
- **Internal**: `./config` module being tested, `./types` for LLMSettings type
- **External**: Jest testing framework

## Key Test Scenarios
- Verifying supported providers (OpenAI, Anthropic, Gemini) are correctly identified
- Ensuring invalid or empty provider/model IDs return appropriate undefined/empty values
- Validating all LLM setting parameters against their defined bounds (temperature 0-2, maxTokens 1-100000, etc.)
- Testing Gemini-specific safety settings validation
- Confirming multiple validation errors are collected and returned together

## Usage Examples
```typescript
// Running specific test suites
npm test -- config.test.ts

// Example of extending tests for a new provider
describe('getProviderById', () => {
  it('should return provider info for new provider', () => {
    const provider = getProviderById('new-provider');
    expect(provider?.id).toBe('new-provider');
    expect(provider?.name).toBe('New Provider');
  });
});
```

## See Also
- `./config.ts` - Implementation file being tested
- `./types.ts` - Type definitions for LLMSettings and related interfaces
- Provider adapter tests for integration testing

## Architecture
The llm directory implements a sophisticated multi-layer architecture following the refactoring:
1. **Orchestration Layer** (LLMService): Coordinates all operations through specialized services
2. **Service Layer** (services/): Focused services handling specific responsibilities:
   - PresetManager: Model preset management
   - AdapterRegistry: Provider adapter lifecycle
   - RequestValidator: Input validation
   - SettingsManager: Settings merging and filtering
   - ModelResolver: Model and preset resolution
3. **Configuration Layer** (config): Manages provider/model metadata and settings hierarchies
4. **Type Layer** (types): Defines contracts for all LLM interactions
5. **Adapter Layer** (clients/): Provider-specific implementations (in subdirectory)

This refactored architecture provides:
- **Single Responsibility**: Each service handles one specific concern
- **Improved Testability**: Services can be tested in isolation
- **Better Maintainability**: Changes are localized to specific services
- **Clear Dependencies**: Explicit service dependencies in LLMService constructor
- **Flexible Extension**: New features can be added as new services

## Internal Dependencies
- LLMService depends on all service classes in services/, types, and config
- Service classes depend on types and config for their specific needs
- Config imports adapter constructors for dynamic registration
- Types module has no internal dependencies
- Test files depend on their respective implementation files
- Services have minimal cross-dependencies, promoting loose coupling

## External Dependencies
- No direct external dependencies at this level
- Individual adapters manage their provider-specific SDK dependencies

## Integration Points
The llm directory serves as the core integration hub:
- **Upward**: Exports LLMService through src/index.ts as the main public API
- **Downward**: Integrates with client adapters in the clients/ subdirectory
- **Lateral**: Uses ApiKeyProvider from parent types module
- **Configuration**: Environment variables can override adapter configurations

## Usage Examples
```typescript
// Complete example showing the integration of all components
import { LLMService, fromEnvironment } from 'genai-lite';

// Initialize service with configuration-driven defaults
const service = new LLMService(fromEnvironment);

// Query available providers and models
const providers = await service.getProviders();
const claudeModels = await service.getModels('anthropic');

// Send a message with model-specific settings
const response = await service.sendMessage({
  providerId: 'anthropic',
  modelId: 'claude-3-5-sonnet-20241022',
  messages: [
    { role: 'user', content: 'Explain quantum computing' }
  ],
  settings: {
    temperature: 0.5,
    maxTokens: 2000
  }
});
```

## See Also
- **Parent Directory**: src/ - Contains main entry point and core types
- **Subdirectories**:
  - src/llm/services/ - Refactored service classes handling specific responsibilities
  - src/llm/clients/ - Provider-specific adapter implementations
- **Related Directories**:
  - src/providers/ - API key provider implementations
  - src/prompting/ - Prompt engineering utilities
- **Key Consumers**: All applications using genai-lite interact primarily through this directory's exports