# Directory: src/llm/

## Overview
The llm directory forms the core of the genai-lite library, housing the main LLM service implementation, comprehensive type definitions, and a sophisticated configuration system. This directory orchestrates interactions with multiple AI providers (OpenAI, Anthropic, Google Gemini, Mistral) through a unified interface while maintaining provider-specific customizations and capabilities. The architecture follows best practices with clear separation between service logic, configuration, and type definitions, all backed by thorough test coverage.

## Key Components
### LLMService.ts
## Overview
The LLMService class serves as the central orchestrator for all LLM operations in the genai-lite library. It provides a unified interface for interacting with multiple AI providers (OpenAI, Anthropic, Google Gemini, Mistral) while abstracting away provider-specific implementation details. The service handles API key management through a flexible provider pattern, validates requests, applies model-specific settings, and routes requests to appropriate client adapters.

## Key Components
- **LLMService** (main class): Central service that coordinates all LLM operations
  - `constructor(apiKeyProvider, options?)`: Initializes service with optional preset configuration
  - `getProviders()`: Returns list of supported AI providers
  - `getModels(providerId)`: Returns available models for a specific provider
  - `getPresets()`: Returns configured model presets (default, extended, or replaced)
  - `sendMessage(request)`: Main method for sending chat messages to LLMs
  - `validateRequestStructure()`: Validates request format and message structure
  - `mergeSettingsForModel()`: Combines user settings with model defaults
  - `getClientAdapter()`: Retrieves appropriate adapter for a provider
  - `registerClientAdapter()`: Registers a new client adapter
  - `getProviderSummary()`: Returns summary of available providers and adapters
- **LLMServiceOptions**: Configuration interface for customizing presets
- **PresetMode**: Type defining preset integration strategies ('extend' or 'replace')

## Architecture & Design
- **Adapter Pattern**: Uses provider-specific client adapters (OpenAIClientAdapter, AnthropicClientAdapter, etc.) that implement the ILLMClientAdapter interface
- **Dependency Injection**: Accepts an ApiKeyProvider function in constructor for flexible API key management
- **Configurable Presets**: Supports default presets with optional extension or replacement via custom presets
- **Dynamic Registration**: Automatically instantiates and registers adapters based on configuration at startup
- **Fallback Strategy**: Uses MockClientAdapter for providers without real adapters
- **Settings Merge Strategy**: Three-level settings hierarchy (global defaults → model-specific defaults → user settings)
- **Parameter Filtering**: Removes unsupported parameters based on provider/model configuration before sending requests

## Dependencies
- **Internal**: 
  - `../types` (ApiKeyProvider type)
  - `./types` (LLM request/response types)
  - `./clients/types` (adapter interfaces)
  - `./config` (provider/model configurations and validation)
  - `./clients/*ClientAdapter` (provider-specific adapters)
  
- **External**: None directly (adapters handle external dependencies)

## Integration Points
- **API Key Management**: Integrates with any ApiKeyProvider implementation for secure key retrieval
- **Client Adapters**: Connects to provider-specific adapters (OpenAI, Anthropic, Gemini, Mistral)
- **Configuration System**: Uses centralized config for provider/model definitions and validation
- **Error Handling**: Provides standardized error responses across all providers

## Usage Examples
```typescript
// Initialize with environment variable provider and default presets
const apiKeyProvider = async (providerId: string) => {
  return process.env[`${providerId.toUpperCase()}_API_KEY`];
};
const llmService = new LLMService(apiKeyProvider);

// Initialize with custom presets (extend mode)
const serviceWithExtended = new LLMService(apiKeyProvider, {
  presets: [{ id: 'custom-gpt4', displayName: 'GPT-4 Custom', providerId: 'openai', modelId: 'gpt-4', settings: { temperature: 0.5 } }],
  presetMode: 'extend'
});

// Initialize with only custom presets (replace mode)
const serviceWithReplaced = new LLMService(apiKeyProvider, {
  presets: myCustomPresets,
  presetMode: 'replace'
});

// Send a message to OpenAI
const response = await llmService.sendMessage({
  providerId: 'openai',
  modelId: 'gpt-4',
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'Hello, how are you?' }
  ],
  settings: {
    temperature: 0.7,
    maxTokens: 1000
  }
});

// Get available models for a provider
const models = await llmService.getModels('anthropic');
```

## See Also
- `src/llm/config.ts` - Provider and model configurations
- `src/llm/clients/*ClientAdapter.ts` - Provider-specific adapter implementations
- `src/providers/fromEnvironment.ts` - Built-in environment variable API key provider
- `src/index.ts` - Main library entry point that exports LLMService

### types.ts
## Overview
This file serves as the central type definition module for the LLM (Large Language Model) interaction system in genai-lite. It provides a comprehensive set of TypeScript interfaces and types that define the structure of all LLM-related data, from basic message formats to complex provider configurations and safety settings.

## Key Components
- **LLMMessage & LLMMessageRole**: Core message structure with role-based conversation modeling (user/assistant/system)
- **LLMChatRequest**: Main request interface combining provider ID, model ID, messages, and optional settings
- **LLMResponse & LLMFailureResponse**: Success and error response structures with detailed metadata
- **LLMSettings**: Comprehensive configuration options including temperature, maxTokens, penalties, and provider-specific settings
- **LLMReasoningSettings**: Configuration for thinking/reasoning modes with enabled, effort, maxTokens, and exclude options
- **ProviderInfo & ModelInfo**: Metadata interfaces for AI providers and their models
- **ModelReasoningCapabilities**: Defines reasoning support, token budgets, and output types for thinking models
- **GeminiSafetySetting**: Google Gemini-specific safety configuration with harm categories and thresholds
- **LLMUsage**: Token usage tracking for billing and quota management
- **LLM_IPC_CHANNELS**: Constants for Inter-Process Communication in Electron contexts

## Architecture & Design
- Uses discriminated unions for response types (success vs error) via the `object` field
- Implements provider-agnostic base types with optional provider-specific extensions
- Supports flexible safety configurations through optional Gemini-specific settings
- Designed for extensibility with optional fields for future provider requirements
- Clear separation between request, response, and configuration concerns

## Dependencies
- **Internal**: None - this is a pure type definition file
- **External**: None - uses only TypeScript built-in types

## Integration Points
This type module serves as the contract between:
- LLM service implementations and client adapters
- API request builders and response parsers
- Configuration systems and runtime validators
- IPC communication layers (for Electron-based consumers)

## Usage Examples
```typescript
// Creating an LLM chat request
const request: LLMChatRequest = {
  providerId: 'openai',
  modelId: 'gpt-4',
  messages: [
    { role: 'system', content: 'You are a helpful assistant' },
    { role: 'user', content: 'Hello!' }
  ],
  settings: {
    temperature: 0.7,
    maxTokens: 1000
  }
};

// Handling responses with type guards
function handleResponse(response: LLMResponse | LLMFailureResponse) {
  if (response.object === 'error') {
    console.error('LLM Error:', response.error.message);
  } else {
    console.log('Assistant:', response.choices[0].message.content);
  }
}
```

## See Also
- `src/llm/clients/types.ts` - Client adapter interfaces that use these types
- `src/llm/LLMService.ts` - Main service implementation consuming these types
- `src/llm/config.ts` - Provider and model configurations using ModelInfo/ProviderInfo
- Provider-specific adapters that implement request/response transformations

### config.ts
## Overview
The config.ts file serves as the central configuration registry for the genai-lite library, defining all supported LLM providers (OpenAI, Anthropic, Gemini, Mistral) and their models with detailed specifications. It manages adapter mappings, default settings, model capabilities, pricing information, and provides utility functions for querying and validating LLM configurations.

## Key Components
- **ADAPTER_CONSTRUCTORS**: Maps provider IDs to their client adapter class constructors
- **ADAPTER_CONFIGS**: Optional configuration objects for each adapter (e.g., custom base URLs)
- **DEFAULT_LLM_SETTINGS**: Global default settings for all LLM requests
- **PROVIDER_DEFAULT_SETTINGS**: Provider-specific setting overrides
- **MODEL_DEFAULT_SETTINGS**: Model-specific setting overrides
- **SUPPORTED_PROVIDERS**: List of all provider configurations
- **SUPPORTED_MODELS**: Comprehensive list of all models with detailed specifications
- **Utility Functions**: getProviderById, getModelById, getModelsByProvider, isProviderSupported, isModelSupported, getDefaultSettingsForModel, validateLLMSettings

## Architecture & Design
- Uses a hierarchical configuration system with three levels of settings: global defaults → provider defaults → model defaults
- Model definitions include comprehensive metadata: pricing, context windows, capabilities, reasoning support, and limitations
- Supports dynamic adapter registration through the constructor mapping pattern
- Implements thorough validation for all LLM settings with specific range checks
- Includes special handling for provider-specific features (e.g., Gemini safety settings, reasoning capabilities)

## Dependencies
- **Internal**: 
  - `./types` - Type definitions for LLM configurations
  - `./clients/types` - ILLMClientAdapter interface
  - `./clients/OpenAIClientAdapter` - OpenAI implementation
  - `./clients/AnthropicClientAdapter` - Anthropic implementation
  - `./clients/GeminiClientAdapter` - Google Gemini implementation
- **External**: None (pure TypeScript configuration)

## Integration Points
- LLMService uses this config to instantiate appropriate adapters via ADAPTER_CONSTRUCTORS
- Client adapters reference model configurations for capability checks
- Validation functions are used by LLMService to ensure valid request parameters
- Environment variables can override adapter base URLs through ADAPTER_CONFIGS

## Usage Examples
```typescript
// Get model information
const model = getModelById('claude-3-5-sonnet-20241022', 'anthropic');
console.log(model.contextWindow); // 200000

// Get all models for a provider
const openaiModels = getModelsByProvider('openai');

// Get merged settings for a specific model
const settings = getDefaultSettingsForModel('o4-mini', 'openai');
// Returns settings with temperature: 1.0 (model override)

// Validate LLM settings
const errors = validateLLMSettings({ temperature: 3.0 });
// Returns ['temperature must be a number between 0 and 2']
```

## See Also
- `src/llm/LLMService.ts` - Main service that consumes this configuration
- `src/llm/clients/*ClientAdapter.ts` - Adapter implementations registered here
- `src/llm/types.ts` - Type definitions used throughout the configuration

### LLMService.test.ts
## Overview
Comprehensive test suite for the LLMService class, which serves as the main entry point for the genai-lite library. The tests verify proper initialization, request validation, error handling, API key management, adapter routing, settings validation, and provider/model information retrieval functionality.

## Test Coverage
- **Constructor and initialization**: Service creation and lazy-loading of client adapters
- **sendMessage validation**: Unsupported providers/models, empty messages, invalid roles, empty content
- **API key handling**: Null keys, provider errors, invalid key formats
- **Adapter routing**: Correct adapter selection, adapter reuse
- **Settings management**: Default settings, user setting merging, temperature/maxTokens/topP validation
- **Provider/model queries**: getProviders() and getModels() functionality

## Testing Approach
The tests use Jest with TypeScript, employing mocked API key providers to isolate the service logic. The test suite uses a clean setup/teardown pattern with `beforeEach` to ensure test isolation. Mock functions are used extensively to simulate API key retrieval without requiring actual API credentials. The tests focus on edge cases and error conditions rather than successful API calls, as network requests are not mocked.

## Dependencies
- **Internal**: LLMService, ApiKeyProvider type, LLM types (LLMChatRequest, LLMResponse, LLMFailureResponse), ADAPTER_ERROR_CODES
- **External**: Jest testing framework with TypeScript support

## Key Test Scenarios
- Validation of all request parameters (provider, model, messages)
- Proper error codes and messages for various failure scenarios
- API key provider error handling (null returns, exceptions)
- Settings validation with boundary testing (temperature ranges, token limits)
- Provider and model enumeration functionality
- Lazy-loading behavior of client adapters

## Usage Examples
```typescript
// Running the tests
npm test src/llm/LLMService.test.ts

// Example of extending tests for a new provider
it('should support new provider', async () => {
  const request: LLMChatRequest = {
    providerId: 'new-provider',
    modelId: 'new-model',
    messages: [{ role: 'user', content: 'Test' }]
  };
  const response = await service.sendMessage(request);
  // Assert expected behavior
});
```

## See Also
- LLMService.ts - The implementation file being tested
- types.ts - Type definitions for ApiKeyProvider
- llm/types.ts - LLM-specific type definitions
- llm/config.ts - Provider and model configurations

### config.test.ts
## Overview
This test file provides comprehensive unit tests for the LLM configuration module (`config.ts`), which manages AI provider and model configurations. It validates all exported functions including provider/model lookup, settings retrieval, and parameter validation to ensure the configuration system works correctly across all supported LLM providers.

## Test Coverage
- **Provider Management**: Tests for `isProviderSupported`, `getProviderById`
- **Model Management**: Tests for `getModelById`, `getModelsByProvider`, `isModelSupported`
- **Settings Management**: Tests for `getDefaultSettingsForModel`, `validateLLMSettings`
- **Edge Cases**: Invalid inputs, unsupported providers/models, boundary value testing

## Testing Approach
The tests use Jest as the testing framework with a describe/it structure for clear organization. Each function is tested in isolation with both positive and negative test cases. No mocking is used as the config module contains pure functions without external dependencies. Tests focus on verifying correct return values, type safety, and proper handling of invalid inputs.

## Dependencies
- **Internal**: `./config` module being tested, `./types` for LLMSettings type
- **External**: Jest testing framework

## Key Test Scenarios
- Verifying supported providers (OpenAI, Anthropic, Gemini) are correctly identified
- Ensuring invalid or empty provider/model IDs return appropriate undefined/empty values
- Validating all LLM setting parameters against their defined bounds (temperature 0-2, maxTokens 1-100000, etc.)
- Testing Gemini-specific safety settings validation
- Confirming multiple validation errors are collected and returned together

## Usage Examples
```typescript
// Running specific test suites
npm test -- config.test.ts

// Example of extending tests for a new provider
describe('getProviderById', () => {
  it('should return provider info for new provider', () => {
    const provider = getProviderById('new-provider');
    expect(provider?.id).toBe('new-provider');
    expect(provider?.name).toBe('New Provider');
  });
});
```

## See Also
- `./config.ts` - Implementation file being tested
- `./types.ts` - Type definitions for LLMSettings and related interfaces
- Provider adapter tests for integration testing

## Architecture
The llm directory implements a sophisticated multi-layer architecture:
1. **Service Layer** (LLMService): Orchestrates all operations and provides the unified interface
2. **Configuration Layer** (config): Manages provider/model metadata and settings hierarchies
3. **Type Layer** (types): Defines contracts for all LLM interactions
4. **Adapter Layer** (clients/): Provider-specific implementations (in subdirectory)

This architecture enables:
- Easy addition of new providers through adapter pattern
- Flexible API key management via dependency injection
- Comprehensive settings management with defaults and overrides
- Type-safe interactions throughout the system

## Internal Dependencies
- LLMService depends on types, config, and client adapters
- Config imports adapter constructors for dynamic registration
- Types module has no internal dependencies
- Test files depend on their respective implementation files

## External Dependencies
- No direct external dependencies at this level
- Individual adapters manage their provider-specific SDK dependencies

## Integration Points
The llm directory serves as the core integration hub:
- **Upward**: Exports LLMService through src/index.ts as the main public API
- **Downward**: Integrates with client adapters in the clients/ subdirectory
- **Lateral**: Uses ApiKeyProvider from parent types module
- **Configuration**: Environment variables can override adapter configurations

## Usage Examples
```typescript
// Complete example showing the integration of all components
import { LLMService, fromEnvironment } from 'genai-lite';

// Initialize service with configuration-driven defaults
const service = new LLMService(fromEnvironment);

// Query available providers and models
const providers = await service.getProviders();
const claudeModels = await service.getModels('anthropic');

// Send a message with model-specific settings
const response = await service.sendMessage({
  providerId: 'anthropic',
  modelId: 'claude-3-5-sonnet-20241022',
  messages: [
    { role: 'user', content: 'Explain quantum computing' }
  ],
  settings: {
    temperature: 0.5,
    maxTokens: 2000
  }
});
```

## See Also
- **Parent Directory**: src/ - Contains main entry point and core types
- **Related Directories**:
  - src/llm/clients/ - Provider-specific adapter implementations
  - src/providers/ - API key provider implementations
  - src/prompting/ - Prompt engineering utilities
- **Key Consumers**: All applications using genai-lite interact primarily through this directory's exports