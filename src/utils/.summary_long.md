# Directory: src/utils/

## Overview
The utils directory provides essential utility functions that support the core functionality of the genai-lite library. It contains utilities for prompt processing including token counting, intelligent text preview generation, and a sophisticated template rendering engine. These utilities are crucial for managing LLM interactions, helping developers understand token usage for cost estimation and rate limiting, providing smart content truncation for better user experiences, and enabling dynamic prompt generation through template rendering.

## Key Components
### index.ts
## Overview
This file serves as the central export point for utility functions in the genai-lite library. It re-exports utilities from the prompt module and template engine module, providing functions for token counting, intelligent text preview generation, and template rendering to help manage LLM interactions and dynamic content generation.

## Key Components
- **Re-exports**: 
  - All functions from `./prompt` module
  - All functions from `./templateEngine` module
  - All functions from `./promptBuilder` module
- **Exported functions**:
  - `countTokens(text: string, model?: TiktokenModel): number` - Counts tokens in text using model-specific tokenizers
  - `getSmartPreview(content: string, config: { minLines: number; maxLines: number }): string` - Generates intelligent content previews
  - `renderTemplate(template: string, variables: Record<string, any>): string` - Renders templates with variable substitution and conditional logic
  - `parseMessagesFromTemplate(template: string, variables?: Record<string, any>): LLMMessage[]` - Parses templates with role tags into message arrays
  - `extractRandomVariables(content: string, options?: { maxPerTag?: number }): Record<string, any>` - Extracts and shuffles random examples
  - `parseStructuredContent(content: string, tags: string[]): Record<string, string>` - Parses structured content from strings

## Architecture & Design
The file follows a barrel export pattern, providing a clean API surface for the utils module. The underlying prompt utilities implement:
- **Tokenizer caching**: Uses a Map to cache tokenizers for performance
- **Graceful fallback**: Falls back to character-based estimation (length/4) if tokenization fails
- **Smart truncation**: Preview generation looks for natural break points (empty lines) within configured bounds

## Dependencies
- **Internal**: `./prompt` module
- **External**: `js-tiktoken` library (used by prompt module for token counting)

## Integration Points
This utility module is likely used throughout the genai-lite library wherever:
- Token counting is needed for API request preparation
- Content needs to be previewed or truncated for display
- Prompt size management is required

## Usage Examples
```typescript
import { countTokens, getSmartPreview } from 'genai-lite/utils';

// Count tokens for GPT-4
const tokenCount = countTokens('Hello, world!', 'gpt-4');

// Generate a smart preview of long content
const preview = getSmartPreview(longText, { 
  minLines: 10, 
  maxLines: 50 
});
```

## See Also
- `src/utils/prompt.ts` - The actual implementation of prompt utilities
- LLM service modules that likely use these utilities for request preparation
- Any UI components that might use getSmartPreview for content display

### prompt.ts
## Overview
This module provides prompt-related utilities focused on token counting and intelligent text preview generation. It implements a cached tokenizer system for efficient token counting across different GPT models and offers a smart preview function that truncates content based on line count and natural break points.

## Key Components
- **tokenizerCache**: Map-based cache storing Tiktoken instances for different models to avoid re-initialization
- **getTokenizer(model)**: Internal function that retrieves or creates cached tokenizer instances for specific models
- **countTokens(text, model)**: Public function that counts tokens in text using the specified model's tokenizer (defaults to 'gpt-4')
- **getSmartPreview(content, config)**: Public function that creates intelligent previews of long content by finding natural break points

## Architecture & Design
The module implements a caching pattern for tokenizer instances to improve performance when counting tokens multiple times. The token counting function includes graceful fallback behavior (estimating 4 characters per token) when tokenization fails. The smart preview algorithm attempts to find natural content boundaries (empty lines) rather than cutting at arbitrary points.

## Dependencies
- **Internal**: None
- **External**: 
  - `js-tiktoken` - OpenAI's tiktoken library for accurate token counting

## Integration Points
This module is designed to be used by other parts of the system that need to:
- Calculate token usage for API rate limiting or cost estimation
- Generate previews of file contents for display in UI or prompts
- Prepare content for LLM consumption with token awareness

## Usage Examples
```typescript
// Count tokens in text
const tokenCount = countTokens("Hello world", "gpt-4");

// Generate a smart preview of long content
const preview = getSmartPreview(longFileContent, {
  minLines: 50,
  maxLines: 100
});
```

## See Also
- Components that build prompts for LLM requests
- Services that need to estimate API costs based on token counts
- UI components displaying file previews

### prompt.test.ts
## Overview
This test file provides comprehensive coverage for the prompt utilities module, which includes token counting functionality using js-tiktoken and smart text preview generation. The tests ensure that token counting works accurately across different LLM models and that the smart preview function intelligently truncates content at logical breakpoints.

## Test Coverage
- **countTokens function**: Tests token counting for various text inputs, different models, edge cases, and fallback behavior
- **getSmartPreview function**: Tests intelligent text truncation logic including handling of empty lines, maxLines limits, and edge cases

## Testing Approach
The tests use Jest with a straightforward unit testing approach, organizing tests into two main describe blocks for each utility function. Tests cover both happy paths and edge cases, including empty inputs, very long text, special characters, and boundary conditions. No mocking is required as the functions are pure utilities.

## Dependencies
- **Internal**: `./prompt` (the implementation file being tested)
- **External**: 
  - `js-tiktoken` (type imports for TiktokenModel)
  - Jest (testing framework)

## Key Test Scenarios
- Token counting with empty strings, simple text, special characters, and emojis
- Model-specific token counting (gpt-4, gpt-3.5-turbo)
- Fallback behavior for invalid models (uses length/4 estimate)
- Smart preview truncation at empty lines within bounds
- Handling of consecutive empty lines and exact boundary cases
- Content truncation with appropriate ellipsis messages

## Usage Examples
```typescript
// Example of testing token counting
const tokenCount = countTokens('Hello, world!', 'gpt-4');

// Example of testing smart preview
const preview = getSmartPreview(longText, { minLines: 5, maxLines: 10 });
```

## See Also
- `./prompt.ts` - Implementation file containing the utilities being tested
- Related prompt generation utilities in the genai-lite library

### templateEngine.ts
## Overview
This module provides a sophisticated micro-templating engine that supports variable substitution and conditional rendering. Originally migrated from the Athanor project, it enables dynamic prompt generation with support for ternary conditionals, multi-line strings in backticks, and intelligent newline handling. The engine is particularly useful for creating complex prompts that adapt based on runtime conditions.

## Key Components
- **renderTemplate(template, variables)**: Main function that processes template strings by substituting variables and evaluating conditional expressions
- **processExpression(expression, variables, leadingNewline, trailingNewline)**: Internal function that handles the actual variable substitution and conditional logic
- **Custom parser**: Implements a depth-aware parser that correctly handles nested `{{}}` patterns within backtick strings

## Architecture & Design
The module uses a custom parsing approach rather than simple regex to handle the complexity of nested template expressions. Key design features:
- **Depth-aware parsing**: Tracks brace depth to correctly match opening and closing `{{}}` pairs
- **Backtick awareness**: Special handling for backtick-delimited strings to allow nested templates
- **Intelligent newline handling**: Removes trailing newlines when substitution results in empty content
- **Recursive processing**: Supports nested variable substitutions within conditional branches

## Supported Syntax
- **Simple substitution**: `{{ variableName }}`
- **Ternary conditionals**: `{{ condition ? `true result` : `false result` }}`
- **Optional false branch**: `{{ condition ? `true result` : `` }}`
- **Nested variables**: `{{ showName ? `Hello, {{name}}!` : `Guest` }}`
- **Multi-line strings**: Backticks preserve newlines and formatting
- **Escaped backticks**: Use `\`` within backtick strings

## Dependencies
- **Internal**: None
- **External**: None (pure JavaScript implementation)

## Integration Points
This template engine is designed for:
- Dynamic prompt generation based on runtime conditions
- Configuration-driven content rendering
- Conditional inclusion of prompt sections
- Any scenario requiring lightweight templating without external dependencies

## Usage Examples
```typescript
import { renderTemplate } from 'genai-lite/utils';

// Simple variable substitution
const greeting = renderTemplate('Hello, {{ name }}!', { name: 'World' });
// Result: "Hello, World!"

// Conditional rendering
const prompt = renderTemplate(
  'Task: {{ task }}\n{{ hasContext ? `Context: {{context}}` : `` }}',
  { task: 'Analyze code', hasContext: true, context: 'TypeScript file' }
);
// Result: "Task: Analyze code\nContext: TypeScript file"

// Complex nested template
const template = `
Project: {{ projectName }}
{{ hasFiles ? `## Files
{{ fileList }}` : `No files selected` }}
`;
const result = renderTemplate(template, {
  projectName: 'MyApp',
  hasFiles: true,
  fileList: '- src/index.ts\n- src/utils.ts'
});
```

## See Also
- `./templateEngine.test.ts` - Comprehensive test suite
- Original implementation in Athanor's promptTemplates.ts

### templateEngine.test.ts
## Overview
This comprehensive test suite validates all aspects of the template engine's functionality, including variable substitution, conditional logic, nested templates, and edge cases. The tests ensure backward compatibility with quote-based syntax while validating the advanced backtick-based features and intelligent newline handling.

## Test Coverage
- **Variable substitution**: Simple replacements, undefined variables, special handling for task_context
- **Conditional logic**: Ternary expressions, optional false branches, nested conditionals
- **Backtick strings**: Multi-line support, escaped backticks, nested variables
- **Newline handling**: Intelligent trimming for empty results, preservation for non-empty results
- **Edge cases**: Empty templates, no placeholders, whitespace handling
- **Backward compatibility**: Quote-based ternary syntax (single and double quotes)

## Testing Approach
The tests use Jest with a comprehensive set of scenarios covering both common use cases and edge conditions. Each test is focused on a specific aspect of the template engine's behavior, making it easy to identify and fix issues. The test suite serves as both validation and documentation of the expected behavior.

## Key Test Scenarios
- Basic variable substitution with various data types
- Ternary conditionals with both branches
- Ternary conditionals with only true branch (empty false)
- Nested variable substitution within conditional branches
- Complex multi-level templates with multiple conditionals
- Special newline handling that removes empty lines
- Backward compatibility with older quote-based syntax

## Dependencies
- **Internal**: `./templateEngine` (implementation being tested)
- **External**: Jest (testing framework)

## See Also
- `./templateEngine.ts` - Implementation file
- Integration tests in applications using the template engine

### promptBuilder.ts
## Overview
This module provides advanced prompt engineering utilities for building and parsing structured prompts. It includes functions for parsing templates with role tags into LLM message arrays, extracting and shuffling random examples for few-shot learning, and parsing structured content from LLM responses. These utilities enable developers to create more sophisticated AI interactions with better control over conversation structure and output parsing.

## Key Components
- **parseMessagesFromTemplate(template, variables?)**: Converts a template string with XML-style role tags into an array of LLMMessage objects
- **extractRandomVariables(content, options?)**: Extracts content from RANDOM_X tags and creates a flattened dictionary with shuffled examples
- **parseStructuredContent(content, tags)**: Parses structured data from strings (typically LLM responses) using custom XML-style tags
- **extractTextAndClean(xmlString, tagName)**: Internal helper that extracts tag content and returns cleaned string
- **extractRandomTags(content)**: Internal helper that finds all RANDOM_X tags and groups their content

## Architecture & Design
The module follows a layered design pattern:
- **Public API Layer**: Three main exported functions with clear TypeScript interfaces
- **Helper Functions**: Internal utilities for XML-style tag parsing and content extraction
- **Integration with Template Engine**: Works seamlessly with the existing renderTemplate function
- **Type Safety**: Full TypeScript support with proper typing for LLMMessage objects

Key design features:
- **XML-style Tag Parsing**: Uses regex-based parsing for flexibility and performance
- **Order Preservation**: Maintains proper ordering of USER/ASSISTANT messages
- **Graceful Handling**: Handles missing tags, unclosed tags, and empty content
- **Random Shuffling**: Implements Fisher-Yates shuffle for random example selection

## Supported Features
- **Role Tags**: `<SYSTEM>`, `<USER>`, `<ASSISTANT>` for building conversations
- **Random Tags**: `<RANDOM_X>` where X can be any identifier for few-shot examples
- **Variable Integration**: Full support for `{{variables}}` within role tags
- **Multi-turn Conversations**: Support for multiple USER and ASSISTANT messages
- **Flexible Tag Parsing**: Works with both closed and unclosed tags

## Dependencies
- **Internal**: 
  - `../llm/types` - For LLMMessage type definitions
  - `./templateEngine` - For renderTemplate integration
- **External**: None (pure TypeScript implementation)

## Integration Points
This module is designed for:
- Building structured conversations from templates
- Implementing few-shot learning with randomized examples
- Parsing structured output from LLM responses
- Creating reusable prompt templates with dynamic content
- Extracting specific sections from AI-generated content

## Usage Examples
```typescript
import { 
  parseMessagesFromTemplate, 
  extractRandomVariables, 
  parseStructuredContent 
} from 'genai-lite/utils';

// Building conversations from templates
const conversationTemplate = `
<SYSTEM>You are an expert {{language}} developer.</SYSTEM>
<USER>Review this code: {{code}}</USER>
<ASSISTANT>I'll analyze your {{language}} code.</ASSISTANT>
<USER>What about performance?</USER>
`;

const messages = parseMessagesFromTemplate(conversationTemplate, {
  language: 'TypeScript',
  code: 'const data = array.filter(x => x > 0).map(x => x * 2);'
});

// Few-shot learning with random examples
const examplesContent = `
<RANDOM_EXAMPLE>Input: 5 + 3\nOutput: 8</RANDOM_EXAMPLE>
<RANDOM_EXAMPLE>Input: 10 - 4\nOutput: 6</RANDOM_EXAMPLE>
<RANDOM_EXAMPLE>Input: 3 * 7\nOutput: 21</RANDOM_EXAMPLE>
`;

const variables = extractRandomVariables(examplesContent, { maxPerTag: 2 });
// Returns: { random_example_1: "...", random_example_2: "...", ... }

// Parsing structured LLM output
const llmResponse = `
<THINKING>
The user wants a factorial function. I should implement it recursively.
</THINKING>

<CODE>
function factorial(n) {
  return n <= 1 ? 1 : n * factorial(n - 1);
}
</CODE>

<EXPLANATION>
This implements factorial using recursion with a base case.
</EXPLANATION>
`;

const parsed = parseStructuredContent(llmResponse, ['THINKING', 'CODE', 'EXPLANATION']);
// Access: parsed.THINKING, parsed.CODE, parsed.EXPLANATION
```

## See Also
- `./promptBuilder.test.ts` - Comprehensive test suite
- `./templateEngine.ts` - Template rendering for variable substitution
- `../llm/types.ts` - LLMMessage type definitions

### promptBuilder.test.ts
## Overview
This comprehensive test suite validates all functionality of the prompt builder utilities, including message parsing from templates, random variable extraction for few-shot learning, and structured content parsing. The tests ensure proper handling of edge cases, maintain type safety, and verify the integration with the template engine.

## Test Coverage
- **parseMessagesFromTemplate**: Template parsing with role tags, variable substitution, message ordering, empty content handling
- **extractRandomVariables**: Random tag extraction, shuffling behavior, maxPerTag limits, multiline content
- **parseStructuredContent**: Closed and unclosed tag parsing, missing tags, ordered extraction, multiline content

## Testing Approach
The tests use Jest with comprehensive coverage of both happy paths and edge cases. Each test focuses on a specific aspect of functionality, making debugging straightforward. The test suite serves as living documentation of expected behavior and usage patterns.

## Key Test Scenarios
- Parsing templates with all role types (SYSTEM, USER, ASSISTANT)
- Handling multiple USER/ASSISTANT messages in correct order
- Variable substitution within role tags including conditionals
- Random variable extraction with shuffling verification
- Structured content parsing with various tag formats
- Edge cases: empty templates, whitespace-only content, missing tags

## Dependencies
- **Internal**: `./promptBuilder` (implementation being tested)
- **External**: Jest (testing framework)

## See Also
- `./promptBuilder.ts` - Implementation file
- Integration examples in the main README.md

## Architecture
The utils directory follows a modular design pattern:
1. **Index Module**: Provides a clean public API through barrel exports
2. **Prompt Module**: Contains token counting and preview generation with performance optimizations
3. **Template Engine Module**: Implements sophisticated template rendering with conditional logic
4. **Prompt Builder Module**: Provides advanced prompt engineering utilities for structured conversations and output parsing
5. **Test Coverage**: Comprehensive tests ensuring reliability of all utility functions

Key architectural decisions:
- **Caching Strategy**: Tokenizer instances are cached to avoid expensive re-initialization
- **Graceful Degradation**: Token counting falls back to character estimation if tiktoken fails
- **Smart Truncation**: Preview generation looks for natural content boundaries rather than arbitrary cuts
- **Custom Parser**: Template engine uses depth-aware parsing to handle nested template expressions
- **Zero Dependencies**: Template engine and prompt builder have no external dependencies, ensuring lightweight integration
- **XML-style Tags**: Prompt builder uses flexible XML-style tags for structured content parsing
- **Modular Integration**: Prompt builder seamlessly integrates with template engine for variable substitution

## Internal Dependencies
- The utils directory is self-contained with no internal dependencies on other genai-lite modules
- This design ensures utilities can be used throughout the library without circular dependencies

## External Dependencies
- `js-tiktoken` - Used for accurate token counting compatible with OpenAI's models
- Jest - Testing framework for the test suite

## Integration Points
The utils module serves as a foundational layer that can be used by:
- **LLM Service**: For token counting before API requests
- **Client Applications**: For displaying content previews and rendering dynamic prompts
- **Cost Estimation**: For calculating API usage costs based on tokens
- **Rate Limiting**: For tracking token usage against limits
- **Prompt Generation**: For creating dynamic, conditional prompts based on runtime data
- **Configuration Systems**: For template-based configuration rendering

## Usage Examples
```typescript
// In an LLM service preparing a request
import { countTokens } from 'genai-lite/utils';

const messageTokens = messages.reduce((total, msg) => 
  total + countTokens(msg.content, modelId), 0
);

if (messageTokens > modelConfig.maxContextTokens) {
  throw new Error('Context length exceeded');
}

// In a UI component showing file preview
import { getSmartPreview } from 'genai-lite/utils';

const FilePreview = ({ content }) => {
  const preview = getSmartPreview(content, {
    minLines: 20,
    maxLines: 50
  });
  return <pre>{preview}</pre>;
};

// Dynamic prompt generation
import { renderTemplate } from 'genai-lite/utils';

const promptTemplate = `
Analyze the following {{ language }} code:
{{ hasContext ? `Context: {{context}}\n` : `` }}
\`\`\`{{ language }}
{{ code }}
\`\`\`
{{ hasInstructions ? `Special instructions: {{instructions}}` : `Please provide a comprehensive analysis.` }}
`;

const prompt = renderTemplate(promptTemplate, {
  language: 'typescript',
  hasContext: true,
  context: 'React component for user authentication',
  code: fileContent,
  hasInstructions: false
});
```

## See Also
- **Parent Directory**: src/ - Main source directory
- **Related Directories**:
  - src/llm/ - May use token counting for request validation
  - src/index.ts - Exports these utilities as part of public API
- **Key Consumers**: Any module needing token counting or content preview functionality