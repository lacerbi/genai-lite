# Directory: src/utils/

## Overview
The utils directory provides essential utility functions that support the core functionality of the genai-lite library. Currently focused on prompt-related utilities, this directory contains functions for accurate token counting using OpenAI's tiktoken library and intelligent text preview generation. These utilities are crucial for managing LLM interactions, helping developers understand token usage for cost estimation and rate limiting, while also providing smart content truncation for better user experiences.

## Key Components
### index.ts
## Overview
This file serves as the central export point for utility functions in the genai-lite library. Currently, it re-exports all utilities from the prompt module, which provides functions for token counting and intelligent text preview generation to help manage LLM prompt sizes and content display.

## Key Components
- **Re-export**: Exports all functions from `./prompt` module
- **Exported functions from prompt module**:
  - `countTokens(text: string, model?: TiktokenModel): number` - Counts tokens in text using model-specific tokenizers
  - `getSmartPreview(content: string, config: { minLines: number; maxLines: number }): string` - Generates intelligent content previews

## Architecture & Design
The file follows a barrel export pattern, providing a clean API surface for the utils module. The underlying prompt utilities implement:
- **Tokenizer caching**: Uses a Map to cache tokenizers for performance
- **Graceful fallback**: Falls back to character-based estimation (length/4) if tokenization fails
- **Smart truncation**: Preview generation looks for natural break points (empty lines) within configured bounds

## Dependencies
- **Internal**: `./prompt` module
- **External**: `js-tiktoken` library (used by prompt module for token counting)

## Integration Points
This utility module is likely used throughout the genai-lite library wherever:
- Token counting is needed for API request preparation
- Content needs to be previewed or truncated for display
- Prompt size management is required

## Usage Examples
```typescript
import { countTokens, getSmartPreview } from 'genai-lite/utils';

// Count tokens for GPT-4
const tokenCount = countTokens('Hello, world!', 'gpt-4');

// Generate a smart preview of long content
const preview = getSmartPreview(longText, { 
  minLines: 10, 
  maxLines: 50 
});
```

## See Also
- `src/utils/prompt.ts` - The actual implementation of prompt utilities
- LLM service modules that likely use these utilities for request preparation
- Any UI components that might use getSmartPreview for content display

### prompt.ts
## Overview
This module provides prompt-related utilities focused on token counting and intelligent text preview generation. It implements a cached tokenizer system for efficient token counting across different GPT models and offers a smart preview function that truncates content based on line count and natural break points.

## Key Components
- **tokenizerCache**: Map-based cache storing Tiktoken instances for different models to avoid re-initialization
- **getTokenizer(model)**: Internal function that retrieves or creates cached tokenizer instances for specific models
- **countTokens(text, model)**: Public function that counts tokens in text using the specified model's tokenizer (defaults to 'gpt-4')
- **getSmartPreview(content, config)**: Public function that creates intelligent previews of long content by finding natural break points

## Architecture & Design
The module implements a caching pattern for tokenizer instances to improve performance when counting tokens multiple times. The token counting function includes graceful fallback behavior (estimating 4 characters per token) when tokenization fails. The smart preview algorithm attempts to find natural content boundaries (empty lines) rather than cutting at arbitrary points.

## Dependencies
- **Internal**: None
- **External**: 
  - `js-tiktoken` - OpenAI's tiktoken library for accurate token counting

## Integration Points
This module is designed to be used by other parts of the system that need to:
- Calculate token usage for API rate limiting or cost estimation
- Generate previews of file contents for display in UI or prompts
- Prepare content for LLM consumption with token awareness

## Usage Examples
```typescript
// Count tokens in text
const tokenCount = countTokens("Hello world", "gpt-4");

// Generate a smart preview of long content
const preview = getSmartPreview(longFileContent, {
  minLines: 50,
  maxLines: 100
});
```

## See Also
- Components that build prompts for LLM requests
- Services that need to estimate API costs based on token counts
- UI components displaying file previews

### prompt.test.ts
## Overview
This test file provides comprehensive coverage for the prompt utilities module, which includes token counting functionality using js-tiktoken and smart text preview generation. The tests ensure that token counting works accurately across different LLM models and that the smart preview function intelligently truncates content at logical breakpoints.

## Test Coverage
- **countTokens function**: Tests token counting for various text inputs, different models, edge cases, and fallback behavior
- **getSmartPreview function**: Tests intelligent text truncation logic including handling of empty lines, maxLines limits, and edge cases

## Testing Approach
The tests use Jest with a straightforward unit testing approach, organizing tests into two main describe blocks for each utility function. Tests cover both happy paths and edge cases, including empty inputs, very long text, special characters, and boundary conditions. No mocking is required as the functions are pure utilities.

## Dependencies
- **Internal**: `./prompt` (the implementation file being tested)
- **External**: 
  - `js-tiktoken` (type imports for TiktokenModel)
  - Jest (testing framework)

## Key Test Scenarios
- Token counting with empty strings, simple text, special characters, and emojis
- Model-specific token counting (gpt-4, gpt-3.5-turbo)
- Fallback behavior for invalid models (uses length/4 estimate)
- Smart preview truncation at empty lines within bounds
- Handling of consecutive empty lines and exact boundary cases
- Content truncation with appropriate ellipsis messages

## Usage Examples
```typescript
// Example of testing token counting
const tokenCount = countTokens('Hello, world!', 'gpt-4');

// Example of testing smart preview
const preview = getSmartPreview(longText, { minLines: 5, maxLines: 10 });
```

## See Also
- `./prompt.ts` - Implementation file containing the utilities being tested
- Related prompt generation utilities in the genai-lite library

## Architecture
The utils directory follows a modular design pattern:
1. **Index Module**: Provides a clean public API through barrel exports
2. **Prompt Module**: Contains the actual implementation with performance optimizations
3. **Test Coverage**: Comprehensive tests ensuring reliability of utility functions

Key architectural decisions:
- **Caching Strategy**: Tokenizer instances are cached to avoid expensive re-initialization
- **Graceful Degradation**: Token counting falls back to character estimation if tiktoken fails
- **Smart Truncation**: Preview generation looks for natural content boundaries rather than arbitrary cuts

## Internal Dependencies
- The utils directory is self-contained with no internal dependencies on other genai-lite modules
- This design ensures utilities can be used throughout the library without circular dependencies

## External Dependencies
- `js-tiktoken` - Used for accurate token counting compatible with OpenAI's models
- Jest - Testing framework for the test suite

## Integration Points
The utils module serves as a foundational layer that can be used by:
- **LLM Service**: For token counting before API requests
- **Client Applications**: For displaying content previews
- **Cost Estimation**: For calculating API usage costs based on tokens
- **Rate Limiting**: For tracking token usage against limits

## Usage Examples
```typescript
// In an LLM service preparing a request
import { countTokens } from 'genai-lite/utils';

const messageTokens = messages.reduce((total, msg) => 
  total + countTokens(msg.content, modelId), 0
);

if (messageTokens > modelConfig.maxContextTokens) {
  throw new Error('Context length exceeded');
}

// In a UI component showing file preview
import { getSmartPreview } from 'genai-lite/utils';

const FilePreview = ({ content }) => {
  const preview = getSmartPreview(content, {
    minLines: 20,
    maxLines: 50
  });
  return <pre>{preview}</pre>;
};
```

## See Also
- **Parent Directory**: src/ - Main source directory
- **Related Directories**:
  - src/llm/ - May use token counting for request validation
  - src/index.ts - Exports these utilities as part of public API
- **Key Consumers**: Any module needing token counting or content preview functionality