# genai-lite Chat Demo

An interactive web application demonstrating the capabilities of the genai-lite library.

## Current Status: Phase 5 Complete âœ…

**Production-ready!** All phases are complete. The application now showcases all genai-lite capabilities with full polish and documentation:

**Core Features:**
- âœ… Express backend with comprehensive API
- âœ… React frontend with Vite + TypeScript
- âœ… Multi-provider support (OpenAI, Anthropic, Gemini, llama.cpp)
- âœ… Full chat interface with message history
- âœ… Advanced settings panel (temperature, reasoning, thinking extraction)
- âœ… Responsive design for desktop and mobile

**Phase 4 - Advanced Features:**
- âœ… Template rendering with 10+ example templates across 4 categories
- âœ… Model preset selection from genai-lite's built-in presets
- âœ… llama.cpp utilities (tokenization, health, embeddings)
- âœ… Advanced features panel with tabbed interface

**Phase 5 - Polish & Production-Ready:**
- âœ… **Settings Persistence**: Automatic save/restore of provider, model, and settings
- âœ… **Export Conversations**: Download as JSON with full metadata
- âœ… **Copy Features**: Copy entire conversation as Markdown or individual messages
- âœ… **Enhanced Error Messages**: Actionable hints for common issues
- âœ… **Loading Animations**: Smooth spinner and transitions
- âœ… **UX Polish**: Button animations, focus states, visual feedback
- âœ… **Comprehensive Documentation**: Complete setup and usage guides

## Features

**Backend (Implemented):**
- âœ… **Multi-Provider Support**: Backend APIs for OpenAI, Anthropic, Google Gemini, Mistral, and llama.cpp
- âœ… **Provider Listing**: API endpoint to list all providers with availability status
- âœ… **Model Listing**: API endpoint to get models for each provider
- âœ… **Chat Completion**: Full chat API with message validation and error handling
- âœ… **Settings Support**: Temperature, maxTokens, topP, reasoning, thinking extraction, and more
- âœ… **llama.cpp Integration**: Local model support without API keys

**Frontend (Implemented):**
- âœ… **Provider Selection**: Switch between AI providers on the fly
- âœ… **Model Selection**: Choose from available models for each provider
- âœ… **Settings Control**: Collapsible panel for adjusting LLM parameters (temperature, maxTokens, topP)
- âœ… **Settings Persistence**: Auto-save/restore preferences to localStorage
- âœ… **Chat Interface**: Interactive message list with auto-scroll and timestamps
- âœ… **Message Input**: Text area with Enter to send, Shift+Enter for newline
- âœ… **Copy Individual Messages**: Quick-copy button on each message
- âœ… **Reasoning Display**: Collapsible sections for reasoning/thinking output
- âœ… **Error Handling**: Actionable error messages with troubleshooting hints
- âœ… **Loading States**: Animated spinner with smooth transitions
- âœ… **Responsive Design**: Works on desktop and mobile devices

**Advanced Features:**
- âœ… **10 Example Templates**: Categorized templates (general, code, creative, analysis)
  - Basic greetings, code review, creative writing, problem-solving
  - Translation with few-shot examples, technical docs, data analysis
  - Debugging helper, interview prep, adaptive learning tutor
  - Category filter and template tags for easy discovery
- âœ… **Template Rendering**: Demonstrate genai-lite's template engine with `createMessages()`
  - Variable substitution and type-aware editing (string, boolean, number)
  - Shows rendered messages, model context, and settings from `<META>` blocks
- âœ… **Model Presets**: Select from built-in genai-lite presets
- âœ… **llama.cpp Utilities**: Local model tools (no API keys needed)
  - Tokenization with token counts
  - Server health checks and slot monitoring
  - Embedding generation for semantic search

**Export & Sharing:**
- âœ… **Export as JSON**: Download full conversation with metadata
- âœ… **Copy as Markdown**: Copy formatted conversation to clipboard
- âœ… **Copy Individual Messages**: Quick-copy any message with reasoning

## Prerequisites

- Node.js 20+ installed
- npm or yarn
- At least one LLM provider API key (optional for Phase 1)

## Quick Start

### 1. Install Dependencies

```bash
cd examples/chat-demo
npm install
```

### 2. Configure Environment Variables

```bash
cp .env.example .env
# Edit .env and add your API keys
```

### 3. Run the Application

```bash
npm run dev
```

This will start:
- Backend server on http://localhost:3000
- Frontend dev server on http://localhost:5173

Visit http://localhost:5173 in your browser.

## Available Scripts

- `npm run dev` - Start both frontend and backend in development mode
- `npm run dev:frontend` - Start only the Vite frontend dev server
- `npm run dev:backend` - Start only the Express backend server
- `npm run build` - Build the frontend for production
- `npm run preview` - Preview the production build

## Project Structure

```
examples/chat-demo/
â”œâ”€â”€ server/                 # Express backend
â”‚   â”œâ”€â”€ index.ts           # Server entry point
â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â””â”€â”€ llm.ts         # LLM service with genai-lite
â”‚   â””â”€â”€ routes/            # API endpoints
â”‚       â”œâ”€â”€ chat.ts        # Chat completion
â”‚       â”œâ”€â”€ providers.ts   # Provider listing
â”‚       â”œâ”€â”€ models.ts      # Model listing
â”‚       â”œâ”€â”€ presets.ts     # Preset listing (Phase 4)
â”‚       â”œâ”€â”€ templates.ts   # Template rendering (Phase 4)
â”‚       â””â”€â”€ llamacpp.ts    # llama.cpp utilities (Phase 4)
â”œâ”€â”€ src/                   # React frontend
â”‚   â”œâ”€â”€ main.tsx          # React entry point
â”‚   â”œâ”€â”€ App.tsx           # Root component
â”‚   â”œâ”€â”€ style.css         # Global styles
â”‚   â”œâ”€â”€ api/              # API client
â”‚   â”‚   â””â”€â”€ client.ts     # Backend API communication
â”‚   â”œâ”€â”€ components/       # React components
â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx    # Main chat orchestrator with persistence
â”‚   â”‚   â”œâ”€â”€ MessageList.tsx      # Message display with copy buttons
â”‚   â”‚   â”œâ”€â”€ MessageInput.tsx     # Input field
â”‚   â”‚   â”œâ”€â”€ ProviderSelector.tsx # Provider/model selection
â”‚   â”‚   â”œâ”€â”€ SettingsPanel.tsx    # Settings controls with reset
â”‚   â”‚   â”œâ”€â”€ TemplateExamples.tsx # Template rendering with 10 examples
â”‚   â”‚   â””â”€â”€ LlamaCppTools.tsx    # llama.cpp utilities
â”‚   â”œâ”€â”€ data/             # Static data (Phase 5)
â”‚   â”‚   â””â”€â”€ exampleTemplates.ts  # 10 categorized example templates
â”‚   â””â”€â”€ types/            # TypeScript types
â”‚       â””â”€â”€ index.ts      # Type definitions
â”œâ”€â”€ package.json          # Dependencies and scripts
â”œâ”€â”€ tsconfig.json         # TypeScript config (frontend)
â”œâ”€â”€ vite.config.ts        # Vite configuration
â”œâ”€â”€ PROGRESS.md           # Implementation progress tracker
â””â”€â”€ README.md             # This file
```

## Development Phases

### Phase 1: Project Setup âœ… (Complete)
- Basic Express backend with health endpoint
- Minimal React frontend
- TypeScript configuration
- Development environment

### Phase 2: Backend API âœ… (Complete)
- âœ… LLM service integration with genai-lite
- âœ… Provider and model endpoints
- âœ… Chat completion endpoint
- âœ… Full request validation
- âœ… Error handling and logging

### Phase 3: Frontend UI âœ… (Complete)
- âœ… Chat interface components
- âœ… Provider/model selectors
- âœ… Settings panel
- âœ… Message list with auto-scroll
- âœ… Message input with keyboard shortcuts
- âœ… Comprehensive CSS styling
- âœ… Responsive design

### Phase 4: Advanced Features âœ… (Complete)
- âœ… Template rendering with 3 example templates
- âœ… Model preset selection
- âœ… llama.cpp tools (tokenization, health, embeddings)
- âœ… Advanced features panel with tabs
- âœ… Full integration with ChatInterface

### Phase 5: Polish & Documentation âœ… (Complete)
- âœ… 10 example templates across 4 categories
- âœ… Settings persistence with localStorage
- âœ… Export as JSON / Copy as Markdown
- âœ… Copy individual messages
- âœ… Enhanced error messages with actionable hints
- âœ… Loading animations and UX polish
- âœ… Comprehensive documentation

## Backend API Endpoints

The backend provides the following REST API endpoints:

**GET /api/health**
- Health check endpoint
- Returns server status and timestamp

**GET /api/providers**
- Lists all AI providers with availability status
- Shows which providers have API keys configured

**GET /api/models/:providerId**
- Lists all models for a specific provider
- Returns model details (pricing, context window, capabilities)

**POST /api/chat**
- Sends a message to an LLM
- Request body:
  ```json
  {
    "providerId": "openai",
    "modelId": "gpt-4.1-mini",
    "messages": [{"role": "user", "content": "Hello!"}],
    "settings": {
      "temperature": 0.7,
      "maxTokens": 1000
    }
  }
  ```
- Returns completion with content, reasoning, and usage stats

**GET /api/presets**
- Lists all configured model presets
- Shows available pre-configured model settings

**POST /api/templates/render**
- Renders a template with variables and model context
- Demonstrates genai-lite's `createMessages()` functionality

**GET /api/llamacpp/health**
- Checks llama.cpp server status and slot availability

**POST /api/llamacpp/tokenize**
- Tokenizes text using llama.cpp's tokenizer

**POST /api/llamacpp/embedding**
- Generates vector embeddings for semantic search

## Environment Variables

Create a `.env` file based on `.env.example`:

```bash
# API Keys (at least one required for cloud providers)
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GEMINI_API_KEY=AIza...
MISTRAL_API_KEY=...

# llama.cpp Configuration (optional)
LLAMACPP_API_BASE_URL=http://localhost:8080

# Server Configuration
PORT=3000
```

## Using llama.cpp (Local Models)

To use local models via llama.cpp:

1. Install llama.cpp and download a GGUF model
2. Start the llama.cpp server:
   ```bash
   llama-server -m /path/to/model.gguf --port 8080
   ```
3. No API key needed - llama.cpp will be available as a provider

## Using the Application

### Chat Interface
1. **Select Provider and Model**: Choose from available AI providers in the top selector
2. **Configure Settings**: Click "âš™ï¸ Settings" to adjust temperature, reasoning, etc.
3. **Send Messages**: Type in the input area and press Enter (Shift+Enter for newline)
4. **View Reasoning**: Click on collapsed reasoning sections to see model's thinking
5. **Copy Messages**: Click the ğŸ“‹ button on any message to copy it

### Export & Persistence
- **Export JSON**: Click "ğŸ’¾ Export" to download conversation with full metadata
- **Copy Markdown**: Click "ğŸ“‹ Copy" to copy formatted conversation to clipboard
- **Auto-Save Settings**: Your provider, model, and settings are automatically saved
- **Reset Settings**: Use "Reset to Defaults" button in the Settings panel

### Template Examples
1. **Open Advanced Features**: Click "ğŸ¯ Advanced Features" at the top
2. **Select Templates Tab**: Browse 10 example templates across 4 categories
3. **Filter by Category**: Use the category dropdown to find specific templates
4. **Edit Variables**: Modify template variables (supports strings, booleans, numbers)
5. **Render Template**: Click "Render Template" to see the result with model context

### llama.cpp Tools
1. **Start llama.cpp Server**: `llama-server -m /path/to/model.gguf --port 8080`
2. **Open Advanced Features**: Click "ğŸ¯ Advanced Features"
3. **Select llama.cpp Tools Tab**: Access tokenization, health checks, and embeddings
4. **No API Keys Needed**: All features work locally without cloud API keys

## Troubleshooting

### Backend Not Starting
- Make sure port 3000 is not in use: `lsof -i :3000`
- Check that all dependencies are installed: `npm install`
- Verify .env file exists with API keys

### Frontend Not Loading
- Verify the backend is running on port 3000
- Check browser console for errors (F12)
- Try clearing browser cache and localStorage
- Ensure you're accessing http://localhost:5173

### CORS Errors
- The Vite dev server proxies API requests to avoid CORS issues
- Make sure you're accessing via http://localhost:5173, not a different port
- Check that backend is running on port 3000

### API Key Errors
The app provides enhanced error messages:
- **Missing API Key**: "Missing or invalid API key for openai. Add your API key to the .env file..."
- **Solution**: Add `OPENAI_API_KEY=your-key` to `.env` and restart server

### llama.cpp Not Working
- **Error**: "llama.cpp server not running..."
- **Solution**: Start server with `llama-server -m /path/to/model.gguf --port 8080`
- Verify server is running: `curl http://localhost:8080/health`

### Rate Limits
- **Error**: "Rate limit exceeded..."
- **Solution**: Wait a few moments or switch to llama.cpp for unlimited local inference

## Contributing

This is an example application for the genai-lite library. To contribute:

1. Follow the implementation phases in PROGRESS.md
2. Test thoroughly before moving to the next phase
3. Update PROGRESS.md as you complete tasks
4. Keep code clean and well-documented

## License

Same as genai-lite parent project (MIT)
